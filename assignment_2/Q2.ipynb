{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNtvb_zpN4H7"
      },
      "source": [
        "# ü¶ú NN-Based Language Model\n",
        "In this excercise we will run a basic RNN based language model and answer some questions about the code. It is advised to use GPU to run the code. First run the code then answer the questions below that require modifying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CTOJyYyujICY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pytorch-tutorial' already exists and is not an empty directory.\n",
            "/home/ziruiqiu/comp691_DL/assignment_2/pytorch-tutorial/tutorials/02-intermediate/language_model\n",
            "--> Device selected: cuda\n"
          ]
        }
      ],
      "source": [
        "#@title üßÆ Imports & Hyperparameter Setup\n",
        "#@markdown Feel free to experiment with the following hyperparameters at your\n",
        "#@markdown leasure. For the purpose of this assignment, leave the default values\n",
        "#@markdown and run the code with these suggested values.\n",
        "# Some part of the code was referenced from below.\n",
        "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
        "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model\n",
        "\n",
        "! git clone https://github.com/yunjey/pytorch-tutorial/\n",
        "%cd pytorch-tutorial/tutorials/02-intermediate/language_model/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "embed_size = 128 #@param {type:\"number\"}\n",
        "hidden_size = 1024 #@param {type:\"number\"}\n",
        "num_layers = 1 #@param {type:\"number\"}\n",
        "num_epochs = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "batch_size = 20 #@param {type:\"number\"}\n",
        "seq_length = 30 #@param {type:\"number\"}\n",
        "learning_rate = 0.002 #@param {type:\"number\"}\n",
        "#@markdown Number of words to be sampled ‚¨áÔ∏è\n",
        "num_samples = 50 #@param {type:\"number\"}  \n",
        "\n",
        "print(f\"--> Device selected: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tzj73P_QeBEA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vcoabulary size: 10000\n",
            "Number of batches: 1549\n"
          ]
        }
      ],
      "source": [
        "from data_utils import Dictionary, Corpus\n",
        "\n",
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('data/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length\n",
        "\n",
        "print(f\"Vcoabulary size: {vocab_size}\")\n",
        "print(f\"Number of batches: {num_batches}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKzalmp8dndK"
      },
      "source": [
        "## ü§ñ Model Definition\n",
        "As you can see below, this model stacks `num_layers` many [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) units vertically to construct our basic RNN-based language model. The diagram below shows a pictorial representation of the model in its simplest form (i.e `num_layers`=1).\n",
        "![Pictorial Representation of The Model](https://upload.wikimedia.org/wikipedia/commons/6/63/Long_Short-Term_Memory.svg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QZTjM5fQri35"
      },
      "outputs": [],
      "source": [
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_fjTZ6wdpae"
      },
      "source": [
        "## üèì Training\n",
        "In this section we will train our model, this should take a couple of minutes! Be patient üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DsaIIUUHjQ5n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step[0/1549], Loss: 9.2110, Perplexity: 10006.35\n",
            "Epoch [1/5], Step[100/1549], Loss: 6.0061, Perplexity: 405.91\n",
            "Epoch [1/5], Step[200/1549], Loss: 5.9207, Perplexity: 372.66\n",
            "Epoch [1/5], Step[300/1549], Loss: 5.7405, Perplexity: 311.21\n",
            "Epoch [1/5], Step[400/1549], Loss: 5.6723, Perplexity: 290.70\n",
            "Epoch [1/5], Step[500/1549], Loss: 5.1448, Perplexity: 171.54\n",
            "Epoch [1/5], Step[600/1549], Loss: 5.1881, Perplexity: 179.12\n",
            "Epoch [1/5], Step[700/1549], Loss: 5.3368, Perplexity: 207.84\n",
            "Epoch [1/5], Step[800/1549], Loss: 5.1846, Perplexity: 178.50\n",
            "Epoch [1/5], Step[900/1549], Loss: 5.0611, Perplexity: 157.77\n",
            "Epoch [1/5], Step[1000/1549], Loss: 5.1044, Perplexity: 164.74\n",
            "Epoch [1/5], Step[1100/1549], Loss: 5.3662, Perplexity: 214.05\n",
            "Epoch [1/5], Step[1200/1549], Loss: 5.1858, Perplexity: 178.72\n",
            "Epoch [1/5], Step[1300/1549], Loss: 5.0935, Perplexity: 162.95\n",
            "Epoch [1/5], Step[1400/1549], Loss: 4.8619, Perplexity: 129.27\n",
            "Epoch [1/5], Step[1500/1549], Loss: 5.1804, Perplexity: 177.75\n",
            "Epoch [2/5], Step[0/1549], Loss: 5.4014, Perplexity: 221.71\n",
            "Epoch [2/5], Step[100/1549], Loss: 4.5836, Perplexity: 97.87\n",
            "Epoch [2/5], Step[200/1549], Loss: 4.6661, Perplexity: 106.28\n",
            "Epoch [2/5], Step[300/1549], Loss: 4.6849, Perplexity: 108.30\n",
            "Epoch [2/5], Step[400/1549], Loss: 4.5155, Perplexity: 91.42\n",
            "Epoch [2/5], Step[500/1549], Loss: 4.1205, Perplexity: 61.59\n",
            "Epoch [2/5], Step[600/1549], Loss: 4.4332, Perplexity: 84.20\n",
            "Epoch [2/5], Step[700/1549], Loss: 4.3894, Perplexity: 80.59\n",
            "Epoch [2/5], Step[800/1549], Loss: 4.4621, Perplexity: 86.67\n",
            "Epoch [2/5], Step[900/1549], Loss: 4.2157, Perplexity: 67.74\n",
            "Epoch [2/5], Step[1000/1549], Loss: 4.3100, Perplexity: 74.44\n",
            "Epoch [2/5], Step[1100/1549], Loss: 4.5330, Perplexity: 93.04\n",
            "Epoch [2/5], Step[1200/1549], Loss: 4.4584, Perplexity: 86.35\n",
            "Epoch [2/5], Step[1300/1549], Loss: 4.2475, Perplexity: 69.93\n",
            "Epoch [2/5], Step[1400/1549], Loss: 3.9780, Perplexity: 53.41\n",
            "Epoch [2/5], Step[1500/1549], Loss: 4.3329, Perplexity: 76.17\n",
            "Epoch [3/5], Step[0/1549], Loss: 4.4511, Perplexity: 85.72\n",
            "Epoch [3/5], Step[100/1549], Loss: 3.8819, Perplexity: 48.52\n",
            "Epoch [3/5], Step[200/1549], Loss: 4.0116, Perplexity: 55.23\n",
            "Epoch [3/5], Step[300/1549], Loss: 3.9971, Perplexity: 54.44\n",
            "Epoch [3/5], Step[400/1549], Loss: 3.7994, Perplexity: 44.67\n",
            "Epoch [3/5], Step[500/1549], Loss: 3.4572, Perplexity: 31.73\n",
            "Epoch [3/5], Step[600/1549], Loss: 3.8080, Perplexity: 45.06\n",
            "Epoch [3/5], Step[700/1549], Loss: 3.6856, Perplexity: 39.87\n",
            "Epoch [3/5], Step[800/1549], Loss: 3.7824, Perplexity: 43.92\n",
            "Epoch [3/5], Step[900/1549], Loss: 3.4949, Perplexity: 32.95\n",
            "Epoch [3/5], Step[1000/1549], Loss: 3.6273, Perplexity: 37.61\n",
            "Epoch [3/5], Step[1100/1549], Loss: 3.7046, Perplexity: 40.64\n",
            "Epoch [3/5], Step[1200/1549], Loss: 3.7758, Perplexity: 43.63\n",
            "Epoch [3/5], Step[1300/1549], Loss: 3.5410, Perplexity: 34.50\n",
            "Epoch [3/5], Step[1400/1549], Loss: 3.2537, Perplexity: 25.89\n",
            "Epoch [3/5], Step[1500/1549], Loss: 3.6146, Perplexity: 37.14\n",
            "Epoch [4/5], Step[0/1549], Loss: 3.9858, Perplexity: 53.83\n",
            "Epoch [4/5], Step[100/1549], Loss: 3.2862, Perplexity: 26.74\n",
            "Epoch [4/5], Step[200/1549], Loss: 3.5104, Perplexity: 33.46\n",
            "Epoch [4/5], Step[300/1549], Loss: 3.4407, Perplexity: 31.21\n",
            "Epoch [4/5], Step[400/1549], Loss: 3.3218, Perplexity: 27.71\n",
            "Epoch [4/5], Step[500/1549], Loss: 3.0026, Perplexity: 20.14\n",
            "Epoch [4/5], Step[600/1549], Loss: 3.3742, Perplexity: 29.20\n",
            "Epoch [4/5], Step[700/1549], Loss: 3.2040, Perplexity: 24.63\n",
            "Epoch [4/5], Step[800/1549], Loss: 3.3092, Perplexity: 27.36\n",
            "Epoch [4/5], Step[900/1549], Loss: 2.9599, Perplexity: 19.30\n",
            "Epoch [4/5], Step[1000/1549], Loss: 3.1727, Perplexity: 23.87\n",
            "Epoch [4/5], Step[1100/1549], Loss: 3.1486, Perplexity: 23.30\n",
            "Epoch [4/5], Step[1200/1549], Loss: 3.3159, Perplexity: 27.55\n",
            "Epoch [4/5], Step[1300/1549], Loss: 3.0685, Perplexity: 21.51\n",
            "Epoch [4/5], Step[1400/1549], Loss: 2.7673, Perplexity: 15.92\n",
            "Epoch [4/5], Step[1500/1549], Loss: 3.1241, Perplexity: 22.74\n",
            "Epoch [5/5], Step[0/1549], Loss: 3.2950, Perplexity: 26.98\n",
            "Epoch [5/5], Step[100/1549], Loss: 2.8890, Perplexity: 17.97\n",
            "Epoch [5/5], Step[200/1549], Loss: 3.1609, Perplexity: 23.59\n",
            "Epoch [5/5], Step[300/1549], Loss: 3.1019, Perplexity: 22.24\n",
            "Epoch [5/5], Step[400/1549], Loss: 2.9931, Perplexity: 19.95\n",
            "Epoch [5/5], Step[500/1549], Loss: 2.6443, Perplexity: 14.07\n",
            "Epoch [5/5], Step[600/1549], Loss: 3.0917, Perplexity: 22.02\n",
            "Epoch [5/5], Step[700/1549], Loss: 2.7978, Perplexity: 16.41\n",
            "Epoch [5/5], Step[800/1549], Loss: 3.0197, Perplexity: 20.49\n",
            "Epoch [5/5], Step[900/1549], Loss: 2.6472, Perplexity: 14.11\n",
            "Epoch [5/5], Step[1000/1549], Loss: 2.8695, Perplexity: 17.63\n",
            "Epoch [5/5], Step[1100/1549], Loss: 2.9239, Perplexity: 18.61\n",
            "Epoch [5/5], Step[1200/1549], Loss: 3.0010, Perplexity: 20.10\n",
            "Epoch [5/5], Step[1300/1549], Loss: 2.7252, Perplexity: 15.26\n",
            "Epoch [5/5], Step[1400/1549], Loss: 2.4565, Perplexity: 11.66\n",
            "Epoch [5/5], Step[1500/1549], Loss: 2.8557, Perplexity: 17.39\n"
          ]
        }
      ],
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states] \n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vy9OJMEXRJs"
      },
      "source": [
        "# ü§î Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhis12qSX-ce"
      },
      "source": [
        "## 1Ô∏è‚É£ Q2.1 Detaching or not? (10 points)\n",
        "The above code implements a version of truncated backpropagation through time. The implementation only requires the `detach()` function (lines 7-9 of the cell) defined above the loop and used once inside the training loop.\n",
        "* Explain the implementation (compared to not using truncated backprop through time).\n",
        "* What does the `detach()` call here achieve? Draw a computational graph. You may choose to answer this question outside the notebook.\n",
        "* When using using line 7-9 we will typically observe less GPU memory being used during training, explain why in your answer.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. When not using truncated backprop through time, the model would backpropagate through the entire sequence, making it computationally expensive and prone to vanishing or exploding gradients. By implementing TBPTT, the model backpropagates through a fixed number of steps (controlled by seq_length), which makes the training more efficient."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Assume the model has one layer and a sequence length of 3. We can represent the graph as follows:\n",
        "![Drag Racing](cg2.png)\n",
        "* In this graph, the LSTM layer receives inputs from the Embedding layer and is connected to the Linear layer. The outputs from the Linear layer are the predictions at each time step. The arrows represent the flow of information and gradients during forward and backward passes.\n",
        "\n",
        "* When we use truncated backpropagation through time, we limit the number of time steps that gradients are backpropagated through. In this example, the gradients would only flow through the LSTM connections up to a certain number of time steps (e.g., 3 steps in this case).\n",
        "\n",
        "* The detach() function is used to stop gradients from flowing further back in time than the specified number of time steps. In this example, if we apply detach() after 3 time steps, the gradients will not be backpropagated beyond the third time step. This reduces the amount of computation and memory required during training and can help prevent the vanishing gradient problem in long sequences."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Detaching the hidden states breaks the computational graph, and the memory associated with the previous states is freed up. When the computational graph is not detached, it keeps track of all the historical states and their gradients, which increases the memory consumption. By using TBPTT and detaching the hidden states, we can prevent excessive memory usage and make the training process more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbyKZiTgahSv"
      },
      "source": [
        "## üîÆ Model Prediction\n",
        "Below we will use our model to generate text sequence!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DxQ13QcIjPE9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "whose <unk> may lead for car passed to remove a little more care said new wage and the supply of russian blood products \n",
            "while they have played a huge <unk> plan says sam <unk> <unk> the president for local regional division which recently has seemed strong for clients described "
          ]
        }
      ],
      "source": [
        "# Sample from the model\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
        "! cat sample.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXsUDt0tbAHM"
      },
      "source": [
        "## 2Ô∏è‚É£ Q2.2 Sampling strategy (7 points)\n",
        "Consider the sampling procedure above. The current code samples a word:\n",
        "```python\n",
        "word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "```\n",
        "in order to feed the model at each output step and feeding those to the next timestep. Copy below the above cell and modify this sampling startegy to use a greedy sampling which selects the highest probability word at each time step to feed as the next input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2BeO7LSWiyIZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for the N <unk> homeless airlines parent ual corp. announced plans to spin off about $ N billion in assets and pay for $ N billion in assets and pay for $ N billion in assets and pay for $ N billion in assets and pay for $ N billion "
          ]
        }
      ],
      "source": [
        "# Sample greedily from the model\n",
        "with torch.no_grad():\n",
        "    with open('sample_greedy.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            word_id = output.argmax(1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample_greedy.txt'))\n",
        "\n",
        "! cat sample_greedy.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8YV7laBe9er"
      },
      "source": [
        "## 3Ô∏è‚É£ Q2.3 Embedding Distance (8 points)\n",
        "Our model has learned a specific set of word embeddings.\n",
        "* Write a function that takes in 2 words and prints the cosine distance between their embeddings using the word embeddings from the above models.\n",
        "* Use it to print the cosine distance of the word \"army\" and the word \"taxpayer\".\n",
        "\n",
        "*Refer to the sampling code for how to output the words corresponding to each index. To get the index you can use the function `corpus.dictionary.word2idx.`*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e6w3JSY3d_6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance between 'army' and 'taxpayer': 1.1329\n"
          ]
        }
      ],
      "source": [
        "# Embedding distance\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def cosine_distance(word1, word2, model):\n",
        "    # Get the word indices\n",
        "    idx1 = corpus.dictionary.word2idx[word1]\n",
        "    idx2 = corpus.dictionary.word2idx[word2]\n",
        "    \n",
        "    # Get the word embeddings from the model\n",
        "    embed1 = model.embed(torch.tensor(idx1).to(device)).detach().cpu()\n",
        "    embed2 = model.embed(torch.tensor(idx2).to(device)).detach().cpu()\n",
        "    \n",
        "    # Calculate the cosine similarity between the two embeddings\n",
        "    cos_sim = F.cosine_similarity(embed1, embed2, dim=0)\n",
        "    \n",
        "    # Calculate the cosine distance as 1 - cosine similarity\n",
        "    cos_dist = 1 - cos_sim\n",
        "    \n",
        "    return cos_dist\n",
        "\n",
        "word1 = \"army\"\n",
        "word2 = \"taxpayer\"\n",
        "cos_dist = cosine_distance(word1, word2, model)\n",
        "print(\"Cosine distance between '{}' and '{}': {:.4f}\".format(word1, word2, cos_dist))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44EBrsQdA4n"
      },
      "source": [
        "## 4Ô∏è‚É£ Q2.4 Teacher Forcing (Extra Credit 2 points)\n",
        "What is teacher forcing?\n",
        "> Teacher forcing works by using the actual or expected output from the training dataset at the current time step $y(t)$ as input in the next time step $X(t+1)$, rather than the output generated by the network.\n",
        "\n",
        "In the `üèì Training` code this is achieved, implicitly, when we pass the entire input sequence (`inputs = ids[:, i:i+seq_length].to(device)`) to the model at once.\n",
        "\n",
        "Copy below the `üèì Training` code and modify it to disable teacher forcing training. Compare the performance of this model, to original model, what can you conclude? (compare perplexity and convergence rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qfgf5pJGfL-D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step[0/1549], Loss: 8.0842, Perplexity: 3242.89\n",
            "Epoch [1/5], Step[100/1549], Loss: 6.4676, Perplexity: 643.94\n",
            "Epoch [1/5], Step[200/1549], Loss: 6.7590, Perplexity: 861.79\n",
            "Epoch [1/5], Step[300/1549], Loss: 6.7512, Perplexity: 855.04\n",
            "Epoch [1/5], Step[400/1549], Loss: 6.7746, Perplexity: 875.33\n",
            "Epoch [1/5], Step[500/1549], Loss: 6.5994, Perplexity: 734.64\n",
            "Epoch [1/5], Step[600/1549], Loss: 6.5636, Perplexity: 708.82\n",
            "Epoch [1/5], Step[700/1549], Loss: 6.8555, Perplexity: 949.06\n",
            "Epoch [1/5], Step[800/1549], Loss: 6.5771, Perplexity: 718.45\n",
            "Epoch [1/5], Step[900/1549], Loss: 6.9208, Perplexity: 1013.09\n",
            "Epoch [1/5], Step[1000/1549], Loss: 6.8166, Perplexity: 912.85\n",
            "Epoch [1/5], Step[1100/1549], Loss: 6.8877, Perplexity: 980.14\n",
            "Epoch [1/5], Step[1200/1549], Loss: 6.6952, Perplexity: 808.48\n",
            "Epoch [1/5], Step[1300/1549], Loss: 7.0503, Perplexity: 1153.23\n",
            "Epoch [1/5], Step[1400/1549], Loss: 6.8114, Perplexity: 908.12\n",
            "Epoch [1/5], Step[1500/1549], Loss: 6.8431, Perplexity: 937.38\n",
            "Epoch [2/5], Step[0/1549], Loss: 7.5211, Perplexity: 1846.59\n",
            "Epoch [2/5], Step[100/1549], Loss: 6.4506, Perplexity: 633.07\n",
            "Epoch [2/5], Step[200/1549], Loss: 6.5268, Perplexity: 683.23\n",
            "Epoch [2/5], Step[300/1549], Loss: 6.5696, Perplexity: 713.12\n",
            "Epoch [2/5], Step[400/1549], Loss: 6.4041, Perplexity: 604.35\n",
            "Epoch [2/5], Step[500/1549], Loss: 6.3281, Perplexity: 560.09\n",
            "Epoch [2/5], Step[600/1549], Loss: 6.3382, Perplexity: 565.80\n",
            "Epoch [2/5], Step[700/1549], Loss: 6.6523, Perplexity: 774.59\n",
            "Epoch [2/5], Step[800/1549], Loss: 6.4703, Perplexity: 645.69\n",
            "Epoch [2/5], Step[900/1549], Loss: 6.6231, Perplexity: 752.29\n",
            "Epoch [2/5], Step[1000/1549], Loss: 6.5259, Perplexity: 682.57\n",
            "Epoch [2/5], Step[1100/1549], Loss: 6.7133, Perplexity: 823.29\n",
            "Epoch [2/5], Step[1200/1549], Loss: 6.5089, Perplexity: 671.07\n",
            "Epoch [2/5], Step[1300/1549], Loss: 6.7575, Perplexity: 860.49\n",
            "Epoch [2/5], Step[1400/1549], Loss: 6.7283, Perplexity: 835.74\n",
            "Epoch [2/5], Step[1500/1549], Loss: 6.6227, Perplexity: 751.99\n",
            "Epoch [3/5], Step[0/1549], Loss: 7.1007, Perplexity: 1212.83\n",
            "Epoch [3/5], Step[100/1549], Loss: 6.3565, Perplexity: 576.21\n",
            "Epoch [3/5], Step[200/1549], Loss: 6.3517, Perplexity: 573.47\n",
            "Epoch [3/5], Step[300/1549], Loss: 6.4020, Perplexity: 603.04\n",
            "Epoch [3/5], Step[400/1549], Loss: 6.2441, Perplexity: 514.96\n",
            "Epoch [3/5], Step[500/1549], Loss: 6.2531, Perplexity: 519.63\n",
            "Epoch [3/5], Step[600/1549], Loss: 6.1925, Perplexity: 489.09\n",
            "Epoch [3/5], Step[700/1549], Loss: 6.6277, Perplexity: 755.72\n",
            "Epoch [3/5], Step[800/1549], Loss: 6.4346, Perplexity: 623.04\n",
            "Epoch [3/5], Step[900/1549], Loss: 6.6060, Perplexity: 739.54\n",
            "Epoch [3/5], Step[1000/1549], Loss: 6.5750, Perplexity: 716.95\n",
            "Epoch [3/5], Step[1100/1549], Loss: 6.6583, Perplexity: 779.22\n",
            "Epoch [3/5], Step[1200/1549], Loss: 6.3720, Perplexity: 585.23\n",
            "Epoch [3/5], Step[1300/1549], Loss: 6.7256, Perplexity: 833.47\n",
            "Epoch [3/5], Step[1400/1549], Loss: 6.7043, Perplexity: 815.90\n",
            "Epoch [3/5], Step[1500/1549], Loss: 6.5914, Perplexity: 728.79\n",
            "Epoch [4/5], Step[0/1549], Loss: 7.0238, Perplexity: 1123.07\n",
            "Epoch [4/5], Step[100/1549], Loss: 6.2243, Perplexity: 504.89\n",
            "Epoch [4/5], Step[200/1549], Loss: 6.2777, Perplexity: 532.57\n",
            "Epoch [4/5], Step[300/1549], Loss: 7.0507, Perplexity: 1153.61\n",
            "Epoch [4/5], Step[400/1549], Loss: 6.3061, Perplexity: 547.91\n",
            "Epoch [4/5], Step[500/1549], Loss: 6.5507, Perplexity: 699.73\n",
            "Epoch [4/5], Step[600/1549], Loss: 6.2567, Perplexity: 521.51\n",
            "Epoch [4/5], Step[700/1549], Loss: 6.5376, Perplexity: 690.66\n",
            "Epoch [4/5], Step[800/1549], Loss: 6.4891, Perplexity: 657.94\n",
            "Epoch [4/5], Step[900/1549], Loss: 6.5230, Perplexity: 680.64\n",
            "Epoch [4/5], Step[1000/1549], Loss: 6.5576, Perplexity: 704.55\n",
            "Epoch [4/5], Step[1100/1549], Loss: 6.5251, Perplexity: 682.06\n",
            "Epoch [4/5], Step[1200/1549], Loss: 6.3016, Perplexity: 545.42\n",
            "Epoch [4/5], Step[1300/1549], Loss: 6.7046, Perplexity: 816.17\n",
            "Epoch [4/5], Step[1400/1549], Loss: 6.6565, Perplexity: 777.81\n",
            "Epoch [4/5], Step[1500/1549], Loss: 6.6073, Perplexity: 740.50\n",
            "Epoch [5/5], Step[0/1549], Loss: 7.0621, Perplexity: 1166.84\n",
            "Epoch [5/5], Step[100/1549], Loss: 6.5392, Perplexity: 691.71\n",
            "Epoch [5/5], Step[200/1549], Loss: 6.2164, Perplexity: 500.90\n",
            "Epoch [5/5], Step[300/1549], Loss: 6.5451, Perplexity: 695.84\n",
            "Epoch [5/5], Step[400/1549], Loss: 6.3245, Perplexity: 558.10\n",
            "Epoch [5/5], Step[500/1549], Loss: 6.1458, Perplexity: 466.74\n",
            "Epoch [5/5], Step[600/1549], Loss: 6.3596, Perplexity: 578.03\n",
            "Epoch [5/5], Step[700/1549], Loss: 6.4869, Perplexity: 656.48\n",
            "Epoch [5/5], Step[800/1549], Loss: 6.3394, Perplexity: 566.45\n",
            "Epoch [5/5], Step[900/1549], Loss: 6.4948, Perplexity: 661.72\n",
            "Epoch [5/5], Step[1000/1549], Loss: 6.4733, Perplexity: 647.60\n",
            "Epoch [5/5], Step[1100/1549], Loss: 6.5639, Perplexity: 709.05\n",
            "Epoch [5/5], Step[1200/1549], Loss: 6.3309, Perplexity: 561.69\n",
            "Epoch [5/5], Step[1300/1549], Loss: 6.6934, Perplexity: 807.08\n",
            "Epoch [5/5], Step[1400/1549], Loss: 6.6332, Perplexity: 759.89\n",
            "Epoch [5/5], Step[1500/1549], Loss: 6.7405, Perplexity: 845.97\n"
          ]
        }
      ],
      "source": [
        "# Training code with Teacher Forcing\n",
        "# Modified RNNLM model for step-by-step training without teacher forcing\n",
        "class RNNLM_no_teacher_forcing(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM_no_teacher_forcing, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out.squeeze(1))\n",
        "        return out, (h, c)\n",
        "\n",
        "model_no_tf = RNNLM_no_teacher_forcing(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_no_tf.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model without teacher forcing\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "\n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+1].to(device)  # Only take the first input of the sequence\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "\n",
        "        # Initialize the cumulative loss for this sequence\n",
        "        cumulative_loss = 0\n",
        "\n",
        "        # Loop through the sequence\n",
        "        for j in range(seq_length):\n",
        "            # Forward pass\n",
        "            states = detach(states)\n",
        "            outputs, states = model_no_tf(inputs, states)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, targets[:, j])\n",
        "            cumulative_loss += loss.item()\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model_no_tf.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Use the predicted output as input for the next time step\n",
        "            inputs = outputs.argmax(dim=1).unsqueeze(1).to(device)\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            avg_loss = cumulative_loss / seq_length\n",
        "            print('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                .format(epoch+1, num_epochs, step, num_batches, avg_loss, np.exp(avg_loss)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH2TG4v3PBOu"
      },
      "source": [
        "## 5Ô∏è‚É£ Q2.5 Distance Comparison (+1 point)\n",
        "Repeat the work you did for `3Ô∏è‚É£ Q2.3 Embedding Distance` for the model in `4Ô∏è‚É£ Q2.4 Teacher Forcing` and compare the distances produced by these two models (i.e. with and without the teacher forcing), what can you conclude?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EABSoOAGPAaS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine distance with teacher forcing: 1.1329\n",
            "Cosine distance without teacher forcing: 1.0003\n"
          ]
        }
      ],
      "source": [
        "word1 = \"army\"\n",
        "word2 = \"taxpayer\"\n",
        "\n",
        "# Calculate cosine distances for both models\n",
        "cos_dist_tf = cosine_distance(word1, word2, model)  # With teacher forcing\n",
        "cos_dist_no_tf = cosine_distance(word1, word2, model_no_tf)  # Without teacher forcing\n",
        "\n",
        "# Print the results\n",
        "print(\"Cosine distance with teacher forcing: {:.4f}\".format(cos_dist_tf))\n",
        "print(\"Cosine distance without teacher forcing: {:.4f}\".format(cos_dist_no_tf))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion:\n",
        "The model with teacher forcing has a smaller cosine distance (1.1329) compared to the model without teacher forcing (1.0003).\n",
        "\n",
        "This suggests that the learned representations of words in the embedding space are influenced by the training method. In this case, the model trained without teacher forcing seems to learn word embeddings that place \"army\" and \"taxpayer\" closer together in the embedding space than the model trained without teacher forcing.\n",
        "\n",
        "It's important to note that this observation is based on a single pair of words, and further analysis would be needed to draw more general conclusions about the effect of teacher forcing on word embeddings. However, this example demonstrates that the choice of training method can have an impact on the learned representations of words."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
