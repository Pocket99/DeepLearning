{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziruiqiu/anaconda3/envs/DL2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import linalg as LA\n",
    "import numpy as np\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 3.398737\n",
      "Epoch 10, Loss 3.397670\n",
      "Epoch 20, Loss 3.396602\n",
      "Epoch 30, Loss 3.395534\n",
      "Epoch 40, Loss 3.394464\n",
      "Epoch 50, Loss 3.393394\n",
      "Epoch 60, Loss 3.392324\n",
      "Epoch 70, Loss 3.391253\n",
      "Epoch 80, Loss 3.390182\n",
      "Epoch 90, Loss 3.389112\n",
      "tensor(0.2334, device='cuda:0')\n",
      "tensor(0.2178, device='cuda:0')\n",
      "tensor(0.0583, device='cuda:0')\n",
      "tensor(0.0430, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device='cuda' \n",
    "\n",
    "# Take ρ(x) = tanh(x). Consider the 1-hidden layer neural network\n",
    "# ŷ i = w 2 T ρ(W 1 x i + b 1 ) + b 2\n",
    "# W is 20 × 10 and w 2 is a vector of size 20. x is a vector of size 10. b 1 and b 2 are vectors of size 20 and 1 respectively.\n",
    "\n",
    "# Define the parameters\n",
    "param_dict = {\n",
    "    'W1': torch.randn(20, 10, device=device, requires_grad=True),\n",
    "    'b1': torch.randn(10, device=device, requires_grad=True),\n",
    "    'W2': torch.randn(20, 20, device=device, requires_grad=True),\n",
    "    'b2': torch.randn(20, device=device, requires_grad=True)\n",
    "}   \n",
    "\n",
    "\n",
    "# Define the network without using any PyTorch modules\n",
    "def my_nn(input, param_dict):\n",
    "\n",
    "    # Reshape the input image from HxW to a flat vector of size H*W\n",
    "    x = input.view(-1, 20)\n",
    "    # print(x.shape)\n",
    "    # print(param_dict['W1'].shape)\n",
    "    # print(param_dict['b1'].shape)\n",
    "    tanh = torch.nn.Tanh()\n",
    "\n",
    "    x = torch.matmul(x, param_dict['W1']) + param_dict['b1']\n",
    "\n",
    "    x = tanh(x)\n",
    "    x = x.view(-1, 20)\n",
    "    # print(x.shape)\n",
    "    # print(param_dict['W2'].shape)\n",
    "    # print(param_dict['b2'].shape)\n",
    "    \n",
    "    x = torch.matmul(x, param_dict['W2']) + param_dict['b2']\n",
    "    x = x.view(-1, 100)\n",
    "    #print(x.shape)\n",
    "    return x\n",
    "\n",
    "# The\n",
    "# absolute loss given\n",
    "# l(ŷ, y) = |ŷ − y|\n",
    "# Consider the cost function J = 1/N ∑ i=1 l(ŷ , y )\n",
    "# Derive an expression for\n",
    "# ∂J/∂W1,∂J/∂W2,∂J/∂b1,∂J/∂b2\n",
    "# and implement it in PyTorch.\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "def my_loss(y_hat, y):\n",
    "    return torch.mean(torch.abs(y_hat - y))\n",
    "\n",
    "# Define the gradient computation by hand\n",
    "def compute_gradients( param_dict, x, y):\n",
    "    W1, b1, W2, b2 = param_dict.values()\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(W1.shape)\n",
    "    print(b1.shape)\n",
    "    print(W2.shape)\n",
    "    print(b2.shape)\n",
    "    \n",
    "    N = x.shape[0]\n",
    "    x = input.view(-1, 20)\n",
    "    z1 = torch.matmul(x ,W1) + b1\n",
    "    a1 = torch.tanh(z1)\n",
    "    a1 = a1.view(-1, 20)\n",
    "    z2 = torch.matmul(a1, W2) + b2\n",
    "    y_pred = z2.view(-1, 100)\n",
    "    \n",
    "    print(y_pred.shape)\n",
    "    sign = torch.sign(y_pred - y)\n",
    "    dz2 = sign / N\n",
    "    a1 = a1.view(-1, 10)\n",
    "    dW2 = torch.matmul(a1, dz2)\n",
    "    db2 = dz2.sum(dim=1)\n",
    "    W2 = W2.view(-1, 10)\n",
    "    da1 = torch.matmul(W2, dz2)\n",
    "\n",
    "    a1 = a1.view(-1, 10)\n",
    "    print(da1.shape)\n",
    "    print((1 - a1**2).shape)\n",
    "    dz1 = da1 * (1 - a1**2)\n",
    "    dW1 = torch.matmul(dz1, x)\n",
    "    db1 = dz1.sum(dim=1)\n",
    "\n",
    "    return np.array([dW1, dW2, db1, db2])\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(param_dict, input, target, num_epochs=100, learning_rate=1e-3):\n",
    "    for epoch in range(num_epochs):\n",
    "        # gradient calulated by hand\n",
    "        # hand_grad = compute_gradients(param_dict, input, target)\n",
    "        # print(hand_grad)\n",
    "\n",
    "        # gradient calculated by pytorch\n",
    "        y_hat = my_nn(input, param_dict)\n",
    "        loss = my_loss(y_hat, target)\n",
    "        loss.backward()\n",
    "\n",
    "        auto_W1_norm = LA.norm(param_dict['W1'].grad)\n",
    "        auto_W2_norm = LA.norm(param_dict['W2'].grad)\n",
    "        auto_b1_norm = LA.norm(param_dict['b1'].grad)\n",
    "        auto_b2_norm = LA.norm(param_dict['b2'].grad)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in param_dict.values():\n",
    "                param -= learning_rate * param.grad\n",
    "                param.grad.zero_()\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    print(auto_W1_norm)\n",
    "    print(auto_W2_norm)\n",
    "    print(auto_b1_norm)\n",
    "    print(auto_b2_norm)\n",
    "    return param_dict\n",
    "\n",
    "# Generate some random data\n",
    "input = torch.randn(100, 20, device=device)\n",
    "target = torch.randn(100, device=device)\n",
    "\n",
    "# Train the network\n",
    "train = train_loop(param_dict, input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 2.186485\n",
      "Epoch 100, Loss 2.001607\n",
      "Epoch 200, Loss 1.813745\n",
      "Epoch 300, Loss 1.624517\n",
      "Epoch 400, Loss 1.448838\n",
      "Epoch 500, Loss 1.301474\n",
      "Epoch 600, Loss 1.186870\n",
      "Epoch 700, Loss 1.100833\n",
      "Epoch 800, Loss 1.037673\n",
      "Epoch 900, Loss 0.992444\n",
      "Test loss: 0.962929\n"
     ]
    }
   ],
   "source": [
    "# Train this model on the sklearn California Housing Prices datasets.\n",
    "# • For this you may use the optimizer and learning rates of your choice and train\n",
    "# for 20-50 epochs.\n",
    "# • Take half the data for training and half for testing.\n",
    "# • Create a validation set from the training set and use it to select a good learning\n",
    "# rate.\n",
    "# • You might want to use the convenient Xavier initialization.\n",
    "# • You are free to use the torch.optim package for this part.\n",
    "# • To speed up things, run the training loop by batches (e.g. 4, 8, 32, 64, etc.).\n",
    "# PyTorch’s DataLoader would be a useful tool to easily fetch a predefined set\n",
    "# of batches per training iteration.\n",
    "# • Report the mean squared error on the train and test set after each epoch.\n",
    "# • You will need to adjust the size of W 1 to fit the size of this data.\n",
    "\n",
    "# Load the data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).float().to(device)\n",
    "\n",
    "# Define the network\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 20)\n",
    "        self.linear2 = torch.nn.Linear(20, 1)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the loss function\n",
    "def my_loss(y_hat, y):\n",
    "    return torch.mean(torch.abs(y_hat - y))\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(model, input, target, num_epochs=1000, learning_rate=1e-3):\n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = model(input)\n",
    "        loss = my_loss(y_hat, target)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "                param.grad.zero_()\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return model\n",
    "\n",
    "# Train the network\n",
    "model = MyNet().to(device)\n",
    "model = train_loop(model, X_train, y_train)\n",
    "\n",
    "# Evaluate the network\n",
    "y_hat = model(X_test)\n",
    "loss = my_loss(y_hat, y_test)\n",
    "print('Test loss: %f' % loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([30, 2])\n",
      "torch.Size([30])\n",
      "torch.Size([30, 30])\n",
      "tensor([[ -1.8950,  -2.7902],\n",
      "        [ -9.1231,  -6.4170],\n",
      "        [-17.9673, -11.5019],\n",
      "        [  3.8187,  -0.6564],\n",
      "        [ -0.0287,  -5.0156],\n",
      "        [ -9.5783,  -7.7911],\n",
      "        [ -6.1696,  -3.3152],\n",
      "        [-12.8570,  -5.3742],\n",
      "        [ -2.7195,   4.2060],\n",
      "        [-14.1800,  -7.3539]])\n"
     ]
    }
   ],
   "source": [
    "# question 2\n",
    "# Consider the neural network\n",
    "# f(x) = WF ρ ◦ WL . . . ρ ◦ Wi . . . ρ ◦ W2ρ ◦ W1x\n",
    "# where W1 is K × D, Wi\n",
    "# is K × K for i > 1, and WF is P × K. Note f : RD → RP .\n",
    "# Take ρ(x) = tanh(x). We will examine different ways to compute the Jacobian ∂f(x)/∂x .\n",
    "# Use torch tensors to write a function which computes the Jacobian,\n",
    "# ∂f(x)/∂x , using backward mode automatic differentiation for a given value of x and\n",
    "# W1, . . . , Wi, . . . , WF where the given matrices are specified by a dictionary of torch\n",
    "# tensors. Implement and test this for L = 3. Your function should only make use\n",
    "# of basic matrix operations (e.g. torch.matmul(), torch.tanh(), etc). You may not\n",
    "# use autograd or autograd.jacobian for your implementation (but you can use them\n",
    "# to unit test your answer). Test it for the case of D = 2, K = 30, P = 10, your\n",
    "# solution does not have to cover all edge cases of K, P,D it is sufficient it works on\n",
    "# the ones provided here.\n",
    "\n",
    "# Define the network\n",
    "def my_nn(x, param_dict):\n",
    "    for i in range(1, L):\n",
    "        x = torch.matmul(param_dict['W' + str(i)], x)\n",
    "        x = torch.tanh(x)\n",
    "    x = torch.matmul(param_dict['WF'], x)\n",
    "    return x\n",
    "\n",
    "# Define the Jacobian function\n",
    "def my_jacobian(x, param_dict):\n",
    "    y = my_nn(x, param_dict)\n",
    "    J = torch.zeros(y.shape[0], x.shape[0])\n",
    "    for i in range(y.shape[0]):\n",
    "        y[i].backward(retain_graph=True)\n",
    "        J[i, :] = x.grad\n",
    "        x.grad.zero_()\n",
    "    return J\n",
    "\n",
    "# Define the parameters\n",
    "D = 2\n",
    "K = 30\n",
    "P = 10\n",
    "L = 3\n",
    "param_dict = {}\n",
    "param_dict['W1'] = torch.randn(K, D, device=device, requires_grad=True)\n",
    "for i in range(2, L):\n",
    "    param_dict['W' + str(i)] = torch.randn(K, K, device=device, requires_grad=True)\n",
    "param_dict['WF'] = torch.randn(P, K, device=device, requires_grad=True)\n",
    "\n",
    "# Define the input\n",
    "x = torch.randn(D, device=device, requires_grad=True)\n",
    "\n",
    "# Compute the Jacobian\n",
    "J = my_jacobian(x, param_dict)\n",
    "print(J)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([30, 2])\n",
      "torch.Size([30])\n",
      "torch.Size([30, 30])\n",
      "tensor([[-3.6832,  3.1388],\n",
      "        [ 2.1358, -3.0074],\n",
      "        [-5.7035,  3.5083],\n",
      "        [ 4.5243, -3.9619],\n",
      "        [ 3.4845, -4.2608],\n",
      "        [-1.6215,  1.7399],\n",
      "        [ 2.4567, -0.5928],\n",
      "        [ 5.4941, -5.6483],\n",
      "        [ 0.4993,  1.7094],\n",
      "        [-5.2610,  3.5687]])\n"
     ]
    }
   ],
   "source": [
    "# (b) (12 points) Implement a function using torch tensors and forward mode automatic\n",
    "# differentiation to compute ∂f(x)/ ∂x . Validate (with assert statements) for several test\n",
    "# cases that your answer matches the function (b) for L = 3. Hint: You must\n",
    "# calculate the derivatives and the network’s output in the same forward pass (unlike\n",
    "# backward differentiation where you need two loops, one for the forward pass and\n",
    "# one for calculating the gradient)\n",
    "\n",
    "# Define the Jacobian function\n",
    "def my_jacobian(x, param_dict):\n",
    "    y = my_nn(x, param_dict)\n",
    "    J = torch.zeros(y.shape[0], x.shape[0])\n",
    "    for i in range(y.shape[0]):\n",
    "        y[i].backward(retain_graph=True)\n",
    "        J[i, :] = x.grad\n",
    "        x.grad.zero_()\n",
    "    return J\n",
    "\n",
    "# Define the parameters\n",
    "D = 2\n",
    "K = 30\n",
    "P = 10\n",
    "L = 3\n",
    "param_dict = {}\n",
    "param_dict['W1'] = torch.randn(K, D, device=device, requires_grad=True)\n",
    "for i in range(2, L):\n",
    "    param_dict['W' + str(i)] = torch.randn(K, K, device=device, requires_grad=True)\n",
    "param_dict['WF'] = torch.randn(P, K, device=device, requires_grad=True)\n",
    "\n",
    "# Define the input\n",
    "x = torch.randn(D, device=device, requires_grad=True)\n",
    "\n",
    "# Compute the Jacobian\n",
    "J = my_jacobian(x, param_dict)\n",
    "print(J)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500c0e9a6935d00759d77722fd6506f0833ee64cfb01e602cf0f077fa4be4564"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
