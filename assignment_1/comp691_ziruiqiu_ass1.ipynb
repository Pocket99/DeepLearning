{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import linalg as LA\n",
    "import numpy as np\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 diff: tensor(1.1259e-07, grad_fn=<LinalgVectorNormBackward0>)\n",
      "b1 diff: tensor(2.8958e-08, grad_fn=<LinalgVectorNormBackward0>)\n",
      "W2 diff: tensor(1.3286e-07, grad_fn=<LinalgVectorNormBackward0>)\n",
      "b2 diff: tensor(2.9802e-08, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device='cuda' \n",
    "\n",
    "\n",
    "# Define network parameters\n",
    "W1 = torch.randn(20, 10, requires_grad=True)\n",
    "b1 = torch.randn(20, 1, requires_grad=True)\n",
    "W2 = torch.randn(20, 1, requires_grad=True)\n",
    "b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Define forward pass function\n",
    "def forward(x):\n",
    "    z = torch.matmul(W1, x) + b1\n",
    "    a = torch.tanh(z)\n",
    "    y_hat = torch.matmul(W2.T, a) + b2\n",
    "    return y_hat\n",
    "\n",
    "# Define loss function\n",
    "def loss(y_hat, y):\n",
    "    return torch.mean(torch.abs(y_hat - y))\n",
    "\n",
    "# Generate random input and output data\n",
    "x = torch.randn(100, 10, 1)\n",
    "y = torch.randn(100, 1, 1)\n",
    "\n",
    "# Compute forward pass and loss\n",
    "y_hat = forward(x)\n",
    "L = loss(y_hat, y)\n",
    "\n",
    "# Compute gradients with autograd\n",
    "L.backward()\n",
    "auto_W1 = W1.grad\n",
    "auto_b1 = b1.grad\n",
    "auto_W2 = W2.grad\n",
    "auto_b2 = b2.grad\n",
    "\n",
    "# Compute gradients manually\n",
    "hand_b2 = torch.mean(torch.sign(y_hat - y), dim=0)\n",
    "hand_W2 = torch.mean(torch.sign(y_hat - y) * torch.tanh(torch.matmul(W1, x) + b1), dim=0)\n",
    "hand_b1 = torch.mean(torch.sign(y_hat - y) * W2 * (1 - torch.tanh(torch.matmul(W1, x) + b1) ** 2), dim=0)\n",
    "hand_W1 = torch.mean(torch.matmul(torch.sign(y_hat - y) * W2 * (1 - torch.tanh(torch.matmul(W1, x) + b1) ** 2), x.permute(0, 2, 1)), dim=0)\n",
    "\n",
    "\n",
    "# Compare gradients\n",
    "print('W1 diff:', torch.linalg.norm(auto_W1 - hand_W1))\n",
    "print('b1 diff:', torch.linalg.norm(auto_b1 - hand_b1))\n",
    "print('W2 diff:', torch.linalg.norm(auto_W2 - hand_W2))\n",
    "print('b2 diff:', torch.linalg.norm(auto_b2 - hand_b2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 2.163523\n",
      "Epoch 5, Loss 2.154351\n",
      "Epoch 10, Loss 2.145171\n",
      "Epoch 15, Loss 2.135983\n",
      "Epoch 20, Loss 2.126787\n",
      "Epoch 25, Loss 2.117583\n",
      "Epoch 30, Loss 2.108370\n",
      "Epoch 35, Loss 2.099148\n",
      "Epoch 40, Loss 2.089918\n",
      "Epoch 45, Loss 2.080679\n",
      "Test loss: 2.079541\n"
     ]
    }
   ],
   "source": [
    "# Train this model on the sklearn California Housing Prices datasets.\n",
    "# • For this you may use the optimizer and learning rates of your choice and train\n",
    "# for 20-50 epochs.\n",
    "# • Take half the data for training and half for testing.\n",
    "# • Create a validation set from the training set and use it to select a good learning\n",
    "# rate.\n",
    "# • You might want to use the convenient Xavier initialization.\n",
    "# • You are free to use the torch.optim package for this part.\n",
    "# • To speed up things, run the training loop by batches (e.g. 4, 8, 32, 64, etc.).\n",
    "# PyTorch’s DataLoader would be a useful tool to easily fetch a predefined set\n",
    "# of batches per training iteration.\n",
    "# • Report the mean squared error on the train and test set after each epoch.\n",
    "# • You will need to adjust the size of W 1 to fit the size of this data.\n",
    "\n",
    "# Load the data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).float().to(device)\n",
    "\n",
    "# Define the network\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 20)\n",
    "        self.linear2 = torch.nn.Linear(20, 1)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the loss function\n",
    "def my_loss(y_hat, y):\n",
    "    return torch.mean(torch.abs(y_hat - y))\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(model, input, target, num_epochs=50, learning_rate=1e-3):\n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = model(input)\n",
    "        loss = my_loss(y_hat, target)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "                param.grad.zero_()\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return model\n",
    "\n",
    "# Train the network\n",
    "model = MyNet().to(device)\n",
    "model = train_loop(model, X_train, y_train)\n",
    "\n",
    "# Evaluate the network\n",
    "y_hat = model(X_test)\n",
    "loss = my_loss(y_hat, y_test)\n",
    "print('Test loss: %f' % loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -3.0562,  -7.1651],\n",
      "        [ -3.1467,  -4.5187],\n",
      "        [ -3.5917,  -3.6186],\n",
      "        [ -2.7348,   0.9770],\n",
      "        [-11.8466, -18.9227],\n",
      "        [ -1.7418,  -1.5030],\n",
      "        [  4.8680,   5.3640],\n",
      "        [ -2.3818,  -4.6550],\n",
      "        [ -3.8549,  -0.2639],\n",
      "        [  6.4082,   2.2213]])\n",
      "tensor([[ -3.0562,  -7.1651],\n",
      "        [ -3.1467,  -4.5187],\n",
      "        [ -3.5917,  -3.6186],\n",
      "        [ -2.7348,   0.9770],\n",
      "        [-11.8466, -18.9227],\n",
      "        [ -1.7418,  -1.5030],\n",
      "        [  4.8680,   5.3640],\n",
      "        [ -2.3818,  -4.6550],\n",
      "        [ -3.8549,  -0.2639],\n",
      "        [  6.4082,   2.2213]])\n"
     ]
    }
   ],
   "source": [
    "# question 2 \n",
    "# Consider the neural network\n",
    "# f(x) = WF ρ ◦ WL . . . ρ ◦ Wi . . . ρ ◦ W2ρ ◦ W1x\n",
    "# where W1 is K × D, Wi\n",
    "# is K × K for i > 1, and WF is P × K. Note f : RD → RP .\n",
    "# Take ρ(x) = tanh(x). We will examine different ways to compute the Jacobian ∂f(x)/∂x .\n",
    "# (a) \n",
    "# Use torch tensors to write a function which computes the Jacobian,\n",
    "# ∂f(x)/∂x , using backward mode automatic differentiation for a given value of x and\n",
    "# W1, . . . , Wi, . . . , WF where the given matrices are specified by a dictionary of torch\n",
    "# tensors. Implement and test this for L = 3. Your function should only make use\n",
    "# of basic matrix operations (e.g. torch.matmul(), torch.tanh(), etc). You may not\n",
    "# use autograd or autograd.jacobian for your implementation (but you can use them\n",
    "# to unit test your answer). Test it for the case of D = 2, K = 30, P = 10, your\n",
    "# solution does not have to cover all edge cases of K, P,D it is sufficient it works on\n",
    "# the ones provided here.\n",
    "\n",
    "# Define the network\n",
    "def my_nn(x, param_dict):\n",
    "    for i in range(1, L):\n",
    "        x = torch.matmul(param_dict['W' + str(i)], x)\n",
    "        x = torch.tanh(x)\n",
    "    x = torch.matmul(param_dict['WF'], x)\n",
    "    return x\n",
    "\n",
    "# Define the Jacobian function\n",
    "def my_jacobian(x, param_dict):\n",
    "    y = my_nn(x, param_dict)\n",
    "    J = torch.zeros(y.shape[0], x.shape[0])\n",
    "    for i in range(y.shape[0]):\n",
    "        y[i].backward(retain_graph=True)\n",
    "        J[i, :] = x.grad\n",
    "        x.grad.zero_()\n",
    "    return J\n",
    "\n",
    "# Define the parameters\n",
    "D = 2\n",
    "K = 30\n",
    "P = 10\n",
    "L = 3\n",
    "param_dict = {}\n",
    "param_dict['W1'] = torch.randn(K, D, device=device, requires_grad=True)\n",
    "for i in range(2, L):\n",
    "    param_dict['W' + str(i)] = torch.randn(K, K, device=device, requires_grad=True)\n",
    "param_dict['WF'] = torch.randn(P, K, device=device, requires_grad=True)\n",
    "\n",
    "# Define the input\n",
    "x = torch.randn(D, device=device, requires_grad=True)\n",
    "\n",
    "# Compute the Jacobian\n",
    "J = my_jacobian(x, param_dict)\n",
    "print(J)\n",
    "\n",
    "# Compare with autograd\n",
    "x = x.reshape(-1, 1)\n",
    "y = my_nn(x, param_dict)\n",
    "y = y.reshape(-1, 1)\n",
    "J_autograd = torch.autograd.functional.jacobian(lambda x: my_nn(x, param_dict), x)\n",
    "print(J)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([30, 2])\n",
      "torch.Size([30])\n",
      "torch.Size([30, 30])\n",
      "tensor([[-3.6832,  3.1388],\n",
      "        [ 2.1358, -3.0074],\n",
      "        [-5.7035,  3.5083],\n",
      "        [ 4.5243, -3.9619],\n",
      "        [ 3.4845, -4.2608],\n",
      "        [-1.6215,  1.7399],\n",
      "        [ 2.4567, -0.5928],\n",
      "        [ 5.4941, -5.6483],\n",
      "        [ 0.4993,  1.7094],\n",
      "        [-5.2610,  3.5687]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) (4 points) Benchmark the Jacobian computation of (b) compared to that of (c)\n",
    "# for L=3,5,10. Report speed of these answers on test cases using GPU and CPU\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "500c0e9a6935d00759d77722fd6506f0833ee64cfb01e602cf0f077fa4be4564"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
