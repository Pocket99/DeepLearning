{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziruiqiu/anaconda3/envs/DL2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import linalg as LA\n",
    "import numpy as np\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 3.370969\n",
      "Epoch 100, Loss 3.360848\n",
      "Epoch 200, Loss 3.350780\n",
      "Epoch 300, Loss 3.340856\n",
      "Epoch 400, Loss 3.330891\n",
      "Epoch 500, Loss 3.320843\n",
      "Epoch 600, Loss 3.310786\n",
      "Epoch 700, Loss 3.300823\n",
      "Epoch 800, Loss 3.290792\n",
      "Epoch 900, Loss 3.280943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': tensor([[ 0.3905, -1.7307, -1.5761, -0.9631, -0.8393,  0.0843,  0.2048,  0.1466,\n",
       "          -0.4340,  0.4377],\n",
       "         [ 0.9929,  0.6389, -1.7927, -1.3507, -0.9371, -0.2019, -0.3487, -0.0751,\n",
       "          -2.4599, -0.8706],\n",
       "         [-1.9312,  1.3581,  1.3343,  0.4263, -0.0698,  0.9423, -0.9671,  0.1911,\n",
       "           2.4683,  1.1250],\n",
       "         [-0.3267, -0.7562, -0.1369,  0.2259, -0.5556,  0.6053,  0.5326,  0.2126,\n",
       "           0.2380, -0.2512],\n",
       "         [-1.1418, -0.8099,  0.8815,  0.4031,  0.8145,  0.3241,  0.8481,  0.6752,\n",
       "           0.5213,  0.2456],\n",
       "         [ 2.1302, -0.7105,  0.5503,  0.3116, -1.5045,  1.4623,  0.5692,  0.0047,\n",
       "          -0.9773, -0.2093],\n",
       "         [-0.1730,  0.9484, -0.4274,  0.6228, -0.6712,  1.2872, -0.4323,  2.0429,\n",
       "           0.3078,  1.3822],\n",
       "         [-0.7166, -0.7394, -0.9673,  0.2533, -0.3730, -0.6409, -2.0040,  1.3424,\n",
       "          -0.1576,  0.7179],\n",
       "         [-0.4161,  1.8600,  1.7107,  0.2197, -2.4808, -0.1957,  0.5282,  0.1225,\n",
       "          -1.2439, -1.7018],\n",
       "         [ 1.2751,  0.8402,  0.0726,  0.5041, -0.3559, -0.2894,  0.8925, -0.4835,\n",
       "          -0.6858, -1.4284],\n",
       "         [ 1.0738, -0.3755, -1.0353, -0.5896,  1.4474, -0.9553, -1.6638,  0.1593,\n",
       "           0.2238, -0.4790],\n",
       "         [ 0.8479,  0.8611, -1.5327, -2.4060, -1.2286, -0.3729, -0.0723, -0.2153,\n",
       "           0.8412,  0.9563],\n",
       "         [ 1.0654, -0.9469, -0.8540,  0.1310, -0.6736,  0.5480,  0.2761,  0.7607,\n",
       "           1.1744,  0.7230],\n",
       "         [-0.5785, -0.4620, -2.6758, -1.8675, -0.1365, -1.2955, -0.9673, -0.7795,\n",
       "          -0.6030,  0.8295],\n",
       "         [ 0.6068,  0.9149,  0.6066, -1.0363, -0.3838,  1.5409, -0.5839, -0.5239,\n",
       "           0.9168,  0.7800],\n",
       "         [ 0.1506, -0.4834, -0.7015, -0.2392, -1.3132, -1.4182, -0.2331,  1.8693,\n",
       "          -0.2607,  0.4195],\n",
       "         [ 0.5966,  0.6738,  0.1414, -0.0247, -0.7637,  1.1921, -1.3393,  0.8536,\n",
       "          -2.0918, -0.5386],\n",
       "         [ 0.6804, -0.1623,  0.5944, -0.7810,  1.3992,  0.5842,  0.6156,  1.5146,\n",
       "          -0.0442,  1.8750],\n",
       "         [-0.7212, -0.2764, -1.7362, -1.1607, -0.3053, -0.1169,  0.8836,  0.5866,\n",
       "          -0.2494, -1.5538],\n",
       "         [-1.3531,  0.5640,  0.7538,  2.2278,  0.8919, -1.5873, -0.0726, -1.6151,\n",
       "          -0.5271,  1.1988]], device='cuda:0', requires_grad=True),\n",
       " 'b1': tensor([ 0.4408, -1.0366, -0.2877, -0.2097,  2.0391, -0.0679,  1.2859, -0.4426,\n",
       "         -0.3853, -0.0414], device='cuda:0', requires_grad=True),\n",
       " 'W2': tensor([[-5.1730e-02,  9.9720e-01, -3.7610e-01, -1.0496e+00, -8.9032e-01,\n",
       "          -3.2471e-01,  4.4258e-01, -1.2168e+00,  4.7562e-02, -8.9998e-01,\n",
       "           1.2776e+00,  1.6064e+00,  3.7741e-01,  7.7780e-01, -7.1151e-01,\n",
       "          -1.3445e+00,  7.9457e-01, -1.0481e+00, -6.3812e-01,  6.8453e-01],\n",
       "         [ 4.6193e-01,  3.4332e-01,  7.0987e-01,  9.4787e-02, -5.4029e-02,\n",
       "           9.8803e-01, -5.5094e-01,  5.7101e-01, -4.6906e-01,  2.3393e-01,\n",
       "           6.5160e-01,  4.0469e-01,  1.9970e-02, -3.4672e-01,  4.8504e-01,\n",
       "           7.7097e-01,  8.5720e-01, -6.8850e-01, -1.9142e+00,  2.1172e+00],\n",
       "         [ 4.1379e-01, -1.2233e+00, -1.1995e+00, -3.8172e-01,  4.3607e-01,\n",
       "          -1.6827e+00, -1.1844e+00, -6.5922e-01,  2.1229e-01,  6.9268e-01,\n",
       "          -8.6160e-01, -8.0325e-01, -1.4133e+00,  1.8195e+00, -2.4164e-01,\n",
       "          -6.8960e-01,  1.9035e+00,  3.6693e-01,  3.3170e+00, -5.8362e-01],\n",
       "         [-3.1500e-01, -1.0163e-01, -3.2997e-01,  2.4653e+00, -1.3052e+00,\n",
       "           2.3120e-01,  5.3952e-02,  2.5444e+00,  6.4609e-01, -3.7021e-01,\n",
       "          -4.9440e-01,  8.1215e-01, -6.5865e-01,  7.8178e-02,  2.6660e-01,\n",
       "           6.6995e-01,  1.7058e+00, -2.8682e-01, -8.6311e-01,  1.0538e-01],\n",
       "         [ 1.3224e+00, -3.6160e-01,  1.5751e+00,  6.0151e-01, -6.6648e-01,\n",
       "           2.5099e+00, -5.2690e-01, -8.4772e-01, -8.3327e-01, -1.3076e+00,\n",
       "           2.9977e-01,  5.2481e-01,  3.5291e-01, -3.8333e-01,  1.9606e-01,\n",
       "          -1.0250e+00, -6.6543e-01, -1.0183e+00, -1.8637e+00,  1.0086e+00],\n",
       "         [ 1.2964e-01,  1.5316e+00, -1.1249e+00,  1.6269e+00, -1.1650e+00,\n",
       "           1.2273e+00,  6.9968e-01,  5.5430e-03,  9.3655e-01,  5.1151e-01,\n",
       "           1.2694e-01, -3.7903e-01,  1.2219e+00, -4.2626e-01, -4.1767e-01,\n",
       "           8.1592e-01,  6.6064e-01,  6.1714e-01, -4.2521e-01,  2.3821e-01],\n",
       "         [-1.0198e-01, -8.4778e-01, -2.1238e+00, -1.0552e+00,  4.7171e-01,\n",
       "           2.4561e-01, -5.4442e-01,  1.7662e-01, -1.9608e+00,  8.8036e-02,\n",
       "          -8.5066e-01, -2.6029e-01,  2.4769e-01,  1.6071e+00, -1.1031e+00,\n",
       "           1.4919e+00,  1.2124e+00,  4.1169e-01, -5.2550e-01, -9.4208e-02],\n",
       "         [-1.4441e+00, -1.1418e+00,  1.4509e+00,  7.5545e-01,  8.7300e-01,\n",
       "          -2.3052e-01,  9.8924e-01, -2.0099e-01, -2.1949e+00,  7.4993e-01,\n",
       "           9.4653e-04, -3.6564e-01,  1.1269e+00, -1.1660e+00,  7.9828e-01,\n",
       "          -9.7645e-01,  5.7073e-01,  9.4624e-01, -1.8578e+00, -1.5032e+00],\n",
       "         [ 3.5834e-01, -7.5313e-01, -7.7823e-03,  7.5920e-02,  1.9504e+00,\n",
       "           1.3332e+00, -1.0269e+00, -1.0801e-01, -1.4466e+00,  4.6621e-02,\n",
       "          -7.7784e-01,  5.2571e-01, -2.4461e+00, -7.3855e-01,  6.8280e-01,\n",
       "          -1.0740e+00,  6.8077e-01, -8.6541e-02, -2.7142e-01, -2.8563e-01],\n",
       "         [-1.4647e-01,  9.1251e-01, -7.0844e-01, -1.4543e+00,  9.0319e-01,\n",
       "          -1.0460e+00, -8.3801e-01,  8.9023e-01,  2.0826e-01,  5.6144e-01,\n",
       "           1.5678e+00, -1.4227e-01, -1.1424e+00,  1.3010e+00,  2.5324e+00,\n",
       "          -2.6487e-01,  1.8375e+00,  2.7381e-01,  9.2191e-01,  1.1228e+00],\n",
       "         [-4.6022e-01, -6.4301e-01,  1.0258e+00, -8.6921e-01,  3.0885e+00,\n",
       "          -1.0232e+00,  5.5444e-01, -1.2557e+00,  3.3397e-02, -1.6363e-01,\n",
       "           1.4560e+00,  8.5036e-01, -8.5988e-01,  1.0870e+00,  1.4167e+00,\n",
       "          -2.1651e-01, -4.2690e-02,  3.3126e-01, -7.3238e-01, -3.1414e-01],\n",
       "         [-2.5749e+00,  1.8401e-01,  8.7511e-01, -8.7586e-01,  1.1970e+00,\n",
       "           8.2303e-01,  2.0363e+00,  8.4555e-01, -4.3156e-01,  9.9635e-01,\n",
       "           1.7102e+00,  4.0013e-01,  4.5200e-02, -3.7710e-01, -1.2515e+00,\n",
       "           5.6539e-01,  1.2213e+00, -4.9834e-01, -1.5617e-01,  4.5414e-03],\n",
       "         [-2.6225e-01, -8.2487e-01, -7.8251e-01,  3.0846e-01, -1.3306e+00,\n",
       "          -1.2443e+00,  6.7089e-01,  1.2147e+00,  6.1522e-02, -8.1489e-01,\n",
       "          -1.0950e-01, -2.5266e-03, -1.1238e+00, -8.8167e-01, -1.1168e+00,\n",
       "          -1.6993e+00,  1.3127e+00, -6.5676e-01,  8.7608e-01, -9.0089e-01],\n",
       "         [-1.7605e-01, -1.9919e-01, -5.4838e-01, -4.2299e-01,  3.2595e-01,\n",
       "          -7.9205e-01,  1.2113e+00, -5.8348e-01,  9.2122e-01, -4.1795e-02,\n",
       "           3.1209e-01, -1.3644e+00,  9.6988e-01,  5.7678e-01,  1.6965e-01,\n",
       "          -7.7680e-01, -9.8835e-01,  8.1067e-01, -1.8983e+00, -7.2074e-01],\n",
       "         [-1.2935e+00,  9.2443e-01, -1.1301e+00,  4.9877e-01,  6.3079e-01,\n",
       "           1.4970e+00, -2.0754e-01, -3.1496e-01,  7.5237e-01, -1.1919e-01,\n",
       "          -2.1524e-01,  1.2664e-01, -3.5699e-01, -1.2034e-02, -2.5168e-01,\n",
       "          -1.2001e+00, -1.9657e-01, -1.1113e+00,  5.8214e-01, -9.5012e-01],\n",
       "         [-8.3265e-02,  1.5856e+00,  6.7034e-01,  2.4815e-01, -1.0629e+00,\n",
       "           1.7378e+00, -6.6433e-01, -4.2693e-01,  2.3674e-01,  7.0712e-01,\n",
       "          -2.3251e-01,  2.7215e-01,  3.6729e-01, -6.4958e-01, -1.1107e-02,\n",
       "          -1.0611e+00, -2.0713e-01,  2.8714e+00,  1.5867e+00,  1.2667e+00],\n",
       "         [ 2.7314e-02,  4.1344e-01, -5.1824e-02,  1.0623e+00, -1.6437e+00,\n",
       "          -5.3356e-02, -3.0288e-01, -2.2197e-01, -8.4038e-03,  1.1416e+00,\n",
       "           2.1045e+00,  2.1859e+00, -7.6450e-01,  3.2278e-01, -7.8597e-02,\n",
       "           2.2239e-01,  1.4957e-01,  6.0834e-01, -5.8423e-01, -5.2663e-01],\n",
       "         [-1.9132e+00,  4.8620e-01, -1.3588e+00,  1.3323e+00,  5.8085e-01,\n",
       "           1.6112e+00,  1.3897e+00, -7.1055e-01, -8.6127e-02,  1.4446e+00,\n",
       "           3.2284e-02, -4.0124e-01,  4.2814e-01, -6.6788e-01, -4.1858e-02,\n",
       "          -1.0613e+00,  2.1276e-01,  1.7488e+00, -1.6644e+00, -1.2559e+00],\n",
       "         [ 7.1650e-02,  5.7016e-01,  1.0430e+00,  1.1186e-01,  3.3802e-01,\n",
       "          -6.2187e-01,  2.8864e-01, -2.2081e-01, -9.3495e-01, -3.1425e-01,\n",
       "           7.6391e-01, -1.7828e+00,  1.7481e+00, -1.2194e+00,  7.7381e-01,\n",
       "          -6.6161e-01,  8.6793e-02,  3.4375e-01,  9.0267e-01, -9.4641e-01],\n",
       "         [-1.6786e+00, -1.0357e+00, -1.4043e+00, -5.6180e-01,  5.6994e-01,\n",
       "           1.5061e+00, -2.1337e-02,  1.5572e-01,  2.2669e-01,  1.4689e+00,\n",
       "           5.5368e-01,  1.0370e+00, -1.4850e+00,  3.7770e-01,  1.8167e+00,\n",
       "           3.1918e-01,  1.5891e+00, -4.8344e-01,  9.6329e-01, -5.3125e-01]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " 'b2': tensor([-1.2767, -1.2019, -1.2797,  0.1884, -0.6324,  0.5414,  0.4119,  0.6508,\n",
       "         -0.0758,  0.5137,  0.3118,  0.9718,  0.8691,  0.4566, -0.4935, -0.1031,\n",
       "          0.3660,  0.0500, -0.2712,  1.1285], device='cuda:0',\n",
       "        requires_grad=True)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='cuda' \n",
    "\n",
    "# Take ρ(x) = tanh(x). Consider the 1-hidden layer neural network\n",
    "# ŷ i = w 2 T ρ(W 1 x i + b 1 ) + b 2\n",
    "# W is 20 × 10 and w 2 is a vector of size 20. x is a vector of size 10. b 1 and b 2 are vectors of size 20 and 1 respectively.\n",
    "\n",
    "# Define the parameters\n",
    "param_dict = {\n",
    "    'W1': torch.randn(20, 10, device=device, requires_grad=True),\n",
    "    'b1': torch.randn(10, device=device, requires_grad=True),\n",
    "    'W2': torch.randn(20, 20, device=device, requires_grad=True),\n",
    "    'b2': torch.randn(20, device=device, requires_grad=True)\n",
    "}   \n",
    "\n",
    "\n",
    "# Define the network without using any PyTorch modules\n",
    "def my_nn(input, param_dict):\n",
    "\n",
    "    # Reshape the input image from HxW to a flat vector of size H*W\n",
    "    x = input.view(-1, 20)\n",
    "    # print(x.shape)\n",
    "    # print(param_dict['W1'].shape)\n",
    "    # print(param_dict['b1'].shape)\n",
    "    tanh = torch.nn.Tanh()\n",
    "\n",
    "    x = torch.matmul(x, param_dict['W1']) + param_dict['b1']\n",
    "\n",
    "    x = tanh(x)\n",
    "    x = x.view(-1, 20)\n",
    "    # print(x.shape)\n",
    "    # print(param_dict['W2'].shape)\n",
    "    # print(param_dict['b2'].shape)\n",
    "    \n",
    "    x = torch.matmul(x, param_dict['W2']) + param_dict['b2']\n",
    "    x = x.view(-1, 100)\n",
    "    #print(x.shape)\n",
    "    return x\n",
    "\n",
    "# The\n",
    "# absolute loss given\n",
    "# l(ŷ, y) = |ŷ − y|\n",
    "# Consider the cost function J = 1/N ∑ i=1 l(ŷ , y )\n",
    "# Derive an expression for\n",
    "# ∂J/∂W1,∂J/∂W2,∂J/∂b1,∂J/∂b2\n",
    "# and implement it in PyTorch.\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "def my_loss(y_hat, y):\n",
    "    return torch.mean(torch.abs(y_hat - y))\n",
    "\n",
    "# Define the gradient computation by hand\n",
    "def compute_gradients( param_dict, x, y):\n",
    "    W1, b1, W2, b2 = param_dict.values()\n",
    "    N = x.shape[0]\n",
    "    x = input.view(-1, 20)\n",
    "    z1 = torch.matmul(W1, x) + b1.unsqueeze(1)\n",
    "    a1 = torch.tanh(z1)\n",
    "    z2 = torch.matmul(W2, a1) + b2\n",
    "    y_pred = z2.squeeze()\n",
    "\n",
    "    sign = torch.sign(y_pred - y)\n",
    "    dz2 = sign / N\n",
    "    dW2 = torch.matmul(a1, dz2.T)\n",
    "    db2 = dz2.sum(dim=1)\n",
    "    da1 = torch.matmul(W2, dz2)\n",
    "    dz1 = da1 * (1 - a1**2)\n",
    "    dW1 = torch.matmul(dz1, x)\n",
    "    db1 = dz1.sum(dim=1)\n",
    "\n",
    "    return np.array([dW1, dW2, db1, db2])\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(param_dict, input, target, num_epochs=1000, learning_rate=1e-3):\n",
    "    for epoch in range(num_epochs):\n",
    "        # gradient calulated by hand\n",
    "        # hand_grad = compute_gradients(param_dict, input, target)\n",
    "        # print(LA.norm(hand_grad))\n",
    "        # gradient calculated by pytorch\n",
    "        y_hat = my_nn(input, param_dict)\n",
    "        loss = my_loss(y_hat, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        #print(LA.norm(param_dict['W1'].grad))\n",
    "        with torch.no_grad():\n",
    "            for param in param_dict.values():\n",
    "                param -= learning_rate * param.grad\n",
    "                param.grad.zero_()\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return param_dict\n",
    "\n",
    "# Generate some random data\n",
    "input = torch.randn(100, 20, device=device)\n",
    "target = torch.randn(100, device=device)\n",
    "\n",
    "# Train the network\n",
    "train_loop(param_dict, input, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 1.979535\n",
      "Epoch 100, Loss 1.810317\n",
      "Epoch 200, Loss 1.645368\n",
      "Epoch 300, Loss 1.494032\n",
      "Epoch 400, Loss 1.362126\n",
      "Epoch 500, Loss 1.251653\n",
      "Epoch 600, Loss 1.161922\n",
      "Epoch 700, Loss 1.091001\n",
      "Epoch 800, Loss 1.036773\n",
      "Epoch 900, Loss 0.996090\n",
      "Test loss: 0.967058\n"
     ]
    }
   ],
   "source": [
    "# Train this model on the sklearn California Housing Prices datasets.\n",
    "# • For this you may use the optimizer and learning rates of your choice and train\n",
    "# for 20-50 epochs.\n",
    "# • Take half the data for training and half for testing.\n",
    "# • Create a validation set from the training set and use it to select a good learning\n",
    "# rate.\n",
    "# • You might want to use the convenient Xavier initialization.\n",
    "# • You are free to use the torch.optim package for this part.\n",
    "# • To speed up things, run the training loop by batches (e.g. 4, 8, 32, 64, etc.).\n",
    "# PyTorch’s DataLoader would be a useful tool to easily fetch a predefined set\n",
    "# of batches per training iteration.\n",
    "# • Report the mean squared error on the train and test set after each epoch.\n",
    "# • You will need to adjust the size of W 1 to fit the size of this data.\n",
    "\n",
    "# Load the data\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "y_test = torch.from_numpy(y_test).float().to(device)\n",
    "\n",
    "# Define the network\n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(8, 20)\n",
    "        self.linear2 = torch.nn.Linear(20, 1)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the loss function\n",
    "def my_loss(y_hat, y):\n",
    "    return torch.mean(torch.abs(y_hat - y))\n",
    "\n",
    "# Define the training loop\n",
    "def train_loop(model, input, target, num_epochs=1000, learning_rate=1e-3):\n",
    "    for epoch in range(num_epochs):\n",
    "        y_hat = model(input)\n",
    "        loss = my_loss(y_hat, target)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                param -= learning_rate * param.grad\n",
    "                param.grad.zero_()\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "    return model\n",
    "\n",
    "# Train the network\n",
    "model = MyNet().to(device)\n",
    "model = train_loop(model, X_train, y_train)\n",
    "\n",
    "# Evaluate the network\n",
    "y_hat = model(X_test)\n",
    "loss = my_loss(y_hat, y_test)\n",
    "print('Test loss: %f' % loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "500c0e9a6935d00759d77722fd6506f0833ee64cfb01e602cf0f077fa4be4564"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
