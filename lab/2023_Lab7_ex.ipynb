{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMvnHJy2VAh7"
      },
      "source": [
        "# Lab 7: Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnc8ygodCf88"
      },
      "source": [
        "This lab covers the following topics:\n",
        "\n",
        "- Gain insight into the self-attention operation using the sequential MNIST example from before.\n",
        "- Gain insight into positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcGa4Zk7CSO6"
      },
      "source": [
        "## 0 Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafW18o5aPa9"
      },
      "source": [
        "Run the code cell below to download the MNIST digits dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hov-36duZyzP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-21 13:37:59--  https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz\n",
            "Resolving activeeon-public.s3.eu-west-2.amazonaws.com (activeeon-public.s3.eu-west-2.amazonaws.com)... 52.95.148.6\n",
            "Connecting to activeeon-public.s3.eu-west-2.amazonaws.com (activeeon-public.s3.eu-west-2.amazonaws.com)|52.95.148.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34812527 (33M) [application/x-gzip]\n",
            "Saving to: ‘MNIST.tar.gz’\n",
            "\n",
            "MNIST.tar.gz        100%[===================>]  33.20M   717KB/s    in 47s     \n",
            "\n",
            "2023-03-21 13:38:46 (726 KB/s) - ‘MNIST.tar.gz’ saved [34812527/34812527]\n",
            "\n",
            "MNIST/\n",
            "MNIST/raw/\n",
            "MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte\n",
            "MNIST/raw/train-images-idx3-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "MNIST/raw/train-images-idx3-ubyte.gz\n",
            "MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "MNIST/raw/train-labels-idx1-ubyte\n",
            "MNIST/raw/t10k-labels-idx1-ubyte\n",
            "MNIST/processed/\n",
            "MNIST/processed/test.pt\n",
            "MNIST/processed/training.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ziruiqiu/anaconda3/envs/DL2/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "!wget -O MNIST.tar.gz https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('./', download=True , transform=transforms.Compose([transforms.ToTensor()]), train=True)\n",
        "train_indices = torch.arange(0, 10000)\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "\n",
        "dataset=torchvision.datasets.MNIST('./', download=True , transform=transforms.Compose([transforms.ToTensor()]), train=False)\n",
        "test_indices = torch.arange(0, 10000)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n",
        "                                          shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ-OsDod2fvG"
      },
      "source": [
        "## Exercise 1: Self-Attention without Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w7IP5RKSqOz"
      },
      "source": [
        "In this section, will implement a very simple model based on self-attention without positional encoding. The model you will implement will consider the input image as a sequence of 28 rows. You may use PyTorch's [`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for this part. Implement a model with the following architecture:\n",
        "\n",
        "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
        "\n",
        "* **Linear 1**: Linear layer which converts input of shape `(sequence_length*batch_size, input_size)` to input of shape `(sequence_length*batch_size, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
        "\n",
        "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`. \n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, embed_dim)` to input of shape `(sequence_length*batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, embed_dim)` to `(batch_size, embed_dim)`\n",
        "\n",
        "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, embed_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n",
        "\n",
        "\n",
        "**NOTE**: Be cautious of correctly permuting and reshaping the input between layers. E.g. if `x` is of shape `(batch_size, sequence_length, input_size)`, note that `x.reshape(batch_size*sequence_length, -1) != x.permute(1,0,2).reshape(batch_size*sequence_length, -1)`. In this example, `x.reshape(batch_size*sequence_length, -1)` has `[batch0_seq0, batch0_seq1, ..., batch1_seq0, batch1_seq1, ...]` format, while `x.permute(1,0,2).reshape(batch_size*sequence_length, -1)` has `[batch0_seq0, batch1_seq0, ..., batch0_seq1, batch1_seq1, ...]` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ppwhZ19Ff9FC"
      },
      "outputs": [],
      "source": [
        "# Self-attention without positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        self.input_size = input_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.avgpool = nn.AvgPool1d(kernel_size=seq_length)\n",
        "        self.linear3 = nn.Linear(embed_dim, num_classes)     \n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape # 64, 28, 28\n",
        "        input=x.reshape(batch_size*sequence_length, -1) # 1792, 28\n",
        "\n",
        "        l1_out=self.linear1(input) # 1792, 64\n",
        "        l1_out=l1_out.reshape(batch_size,sequence_length, -1) # 64, 28, 64\n",
        "        l1_out=l1_out.permute(1,0,2) # 28, 64, 64 \n",
        "\n",
        "        a1_out, _=self.attention(l1_out, l1_out, l1_out)\n",
        "        a1_out=a1_out.permute(1,0,2) # 64, 28, 64\n",
        "        a1_out=a1_out.reshape(batch_size*sequence_length, -1) # 1792, 64 \n",
        "        \n",
        "        relu1_out=self.relu(a1_out) # 1792, 64\n",
        "        \n",
        "        l2_out=self.linear2(relu1_out)\n",
        "\n",
        "        relu2_out=self.relu(l2_out) # 1792, 64 \n",
        "        relu2_out=relu2_out.reshape(batch_size, sequence_length, -1) # 64, 28, 64\n",
        "        relu2_out=relu2_out.permute(1,0,2) # 28, 64, 64\n",
        "\n",
        "        a2_out, _=self.attention(relu2_out, relu2_out, relu2_out) # 1792, 64\n",
        "        a2_out=a2_out.permute(1, 0, 2) # 64, 28, 64\n",
        "        a2_out=a2_out.reshape(batch_size, sequence_length, -1) # 1792, 64\n",
        "        a2_out=a2_out.permute(0, 2, 1) # 64, 64, 28\n",
        "\n",
        "        relu3_out=self.relu(a2_out) # 64, 64, 28\n",
        "        \n",
        "        avgpool_out=self.avgpool(relu3_out).squeeze() # 64, 64\n",
        "        l3_out=self.linear3(avgpool_out) # 64, 10\n",
        "        return l3_out\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zu1c88kgKDZ"
      },
      "source": [
        "Train and evaluate your model by running the cell below. Expect to see  `60-80%` test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "z3FlSD16S8Nh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/8], Step [10/157], Loss: 2.2329\n",
            "Epoch [1/8], Step [20/157], Loss: 2.2536\n",
            "Epoch [1/8], Step [30/157], Loss: 2.0334\n",
            "Epoch [1/8], Step [40/157], Loss: 2.1703\n",
            "Epoch [1/8], Step [50/157], Loss: 2.0529\n",
            "Epoch [1/8], Step [60/157], Loss: 1.9635\n",
            "Epoch [1/8], Step [70/157], Loss: 1.9589\n",
            "Epoch [1/8], Step [80/157], Loss: 1.5411\n",
            "Epoch [1/8], Step [90/157], Loss: 1.9601\n",
            "Epoch [1/8], Step [100/157], Loss: 1.6902\n",
            "Epoch [1/8], Step [110/157], Loss: 1.5693\n",
            "Epoch [1/8], Step [120/157], Loss: 1.4755\n",
            "Epoch [1/8], Step [130/157], Loss: 1.4865\n",
            "Epoch [1/8], Step [140/157], Loss: 1.3998\n",
            "Epoch [1/8], Step [150/157], Loss: 1.3154\n",
            "Epoch [2/8], Step [10/157], Loss: 1.5555\n",
            "Epoch [2/8], Step [20/157], Loss: 1.4212\n",
            "Epoch [2/8], Step [30/157], Loss: 1.1543\n",
            "Epoch [2/8], Step [40/157], Loss: 1.3650\n",
            "Epoch [2/8], Step [50/157], Loss: 1.3075\n",
            "Epoch [2/8], Step [60/157], Loss: 1.3857\n",
            "Epoch [2/8], Step [70/157], Loss: 1.5852\n",
            "Epoch [2/8], Step [80/157], Loss: 1.4893\n",
            "Epoch [2/8], Step [90/157], Loss: 1.4679\n",
            "Epoch [2/8], Step [100/157], Loss: 1.2682\n",
            "Epoch [2/8], Step [110/157], Loss: 1.3742\n",
            "Epoch [2/8], Step [120/157], Loss: 0.8979\n",
            "Epoch [2/8], Step [130/157], Loss: 1.2355\n",
            "Epoch [2/8], Step [140/157], Loss: 1.0357\n",
            "Epoch [2/8], Step [150/157], Loss: 1.0891\n",
            "Epoch [3/8], Step [10/157], Loss: 1.0933\n",
            "Epoch [3/8], Step [20/157], Loss: 0.9731\n",
            "Epoch [3/8], Step [30/157], Loss: 0.8808\n",
            "Epoch [3/8], Step [40/157], Loss: 1.1650\n",
            "Epoch [3/8], Step [50/157], Loss: 1.0106\n",
            "Epoch [3/8], Step [60/157], Loss: 0.6729\n",
            "Epoch [3/8], Step [70/157], Loss: 0.7851\n",
            "Epoch [3/8], Step [80/157], Loss: 0.9127\n",
            "Epoch [3/8], Step [90/157], Loss: 0.9715\n",
            "Epoch [3/8], Step [100/157], Loss: 0.7178\n",
            "Epoch [3/8], Step [110/157], Loss: 0.7671\n",
            "Epoch [3/8], Step [120/157], Loss: 0.8968\n",
            "Epoch [3/8], Step [130/157], Loss: 0.9808\n",
            "Epoch [3/8], Step [140/157], Loss: 0.9363\n",
            "Epoch [3/8], Step [150/157], Loss: 0.8001\n",
            "Epoch [4/8], Step [10/157], Loss: 0.6939\n",
            "Epoch [4/8], Step [20/157], Loss: 0.5057\n",
            "Epoch [4/8], Step [30/157], Loss: 0.8507\n",
            "Epoch [4/8], Step [40/157], Loss: 0.8800\n",
            "Epoch [4/8], Step [50/157], Loss: 0.7528\n",
            "Epoch [4/8], Step [60/157], Loss: 0.6637\n",
            "Epoch [4/8], Step [70/157], Loss: 0.7703\n",
            "Epoch [4/8], Step [80/157], Loss: 0.7926\n",
            "Epoch [4/8], Step [90/157], Loss: 0.7631\n",
            "Epoch [4/8], Step [100/157], Loss: 0.7017\n",
            "Epoch [4/8], Step [110/157], Loss: 0.7269\n",
            "Epoch [4/8], Step [120/157], Loss: 0.7160\n",
            "Epoch [4/8], Step [130/157], Loss: 0.5749\n",
            "Epoch [4/8], Step [140/157], Loss: 0.6693\n",
            "Epoch [4/8], Step [150/157], Loss: 0.6612\n",
            "Epoch [5/8], Step [10/157], Loss: 0.9460\n",
            "Epoch [5/8], Step [20/157], Loss: 0.6572\n",
            "Epoch [5/8], Step [30/157], Loss: 0.7643\n",
            "Epoch [5/8], Step [40/157], Loss: 0.8802\n",
            "Epoch [5/8], Step [50/157], Loss: 0.9343\n",
            "Epoch [5/8], Step [60/157], Loss: 0.4237\n",
            "Epoch [5/8], Step [70/157], Loss: 0.6545\n",
            "Epoch [5/8], Step [80/157], Loss: 0.5541\n",
            "Epoch [5/8], Step [90/157], Loss: 0.6303\n",
            "Epoch [5/8], Step [100/157], Loss: 0.6406\n",
            "Epoch [5/8], Step [110/157], Loss: 0.8309\n",
            "Epoch [5/8], Step [120/157], Loss: 0.3618\n",
            "Epoch [5/8], Step [130/157], Loss: 0.4042\n",
            "Epoch [5/8], Step [140/157], Loss: 0.4704\n",
            "Epoch [5/8], Step [150/157], Loss: 0.6671\n",
            "Epoch [6/8], Step [10/157], Loss: 0.4238\n",
            "Epoch [6/8], Step [20/157], Loss: 0.4956\n",
            "Epoch [6/8], Step [30/157], Loss: 0.7377\n",
            "Epoch [6/8], Step [40/157], Loss: 0.5700\n",
            "Epoch [6/8], Step [50/157], Loss: 0.4547\n",
            "Epoch [6/8], Step [60/157], Loss: 0.9616\n",
            "Epoch [6/8], Step [70/157], Loss: 0.8709\n",
            "Epoch [6/8], Step [80/157], Loss: 0.4768\n",
            "Epoch [6/8], Step [90/157], Loss: 0.5356\n",
            "Epoch [6/8], Step [100/157], Loss: 0.6317\n",
            "Epoch [6/8], Step [110/157], Loss: 0.6358\n",
            "Epoch [6/8], Step [120/157], Loss: 0.6565\n",
            "Epoch [6/8], Step [130/157], Loss: 0.8560\n",
            "Epoch [6/8], Step [140/157], Loss: 0.4965\n",
            "Epoch [6/8], Step [150/157], Loss: 0.6847\n",
            "Epoch [7/8], Step [10/157], Loss: 0.6178\n",
            "Epoch [7/8], Step [20/157], Loss: 0.4543\n",
            "Epoch [7/8], Step [30/157], Loss: 0.5569\n",
            "Epoch [7/8], Step [40/157], Loss: 0.4994\n",
            "Epoch [7/8], Step [50/157], Loss: 0.5710\n",
            "Epoch [7/8], Step [60/157], Loss: 0.5431\n",
            "Epoch [7/8], Step [70/157], Loss: 0.7153\n",
            "Epoch [7/8], Step [80/157], Loss: 0.4249\n",
            "Epoch [7/8], Step [90/157], Loss: 0.4682\n",
            "Epoch [7/8], Step [100/157], Loss: 0.5195\n",
            "Epoch [7/8], Step [110/157], Loss: 0.5253\n",
            "Epoch [7/8], Step [120/157], Loss: 0.5488\n",
            "Epoch [7/8], Step [130/157], Loss: 0.4088\n",
            "Epoch [7/8], Step [140/157], Loss: 0.5550\n",
            "Epoch [7/8], Step [150/157], Loss: 0.4822\n",
            "Epoch [8/8], Step [10/157], Loss: 0.4190\n",
            "Epoch [8/8], Step [20/157], Loss: 0.5732\n",
            "Epoch [8/8], Step [30/157], Loss: 0.5904\n",
            "Epoch [8/8], Step [40/157], Loss: 0.6656\n",
            "Epoch [8/8], Step [50/157], Loss: 0.9752\n",
            "Epoch [8/8], Step [60/157], Loss: 0.3643\n",
            "Epoch [8/8], Step [70/157], Loss: 0.4897\n",
            "Epoch [8/8], Step [80/157], Loss: 0.5713\n",
            "Epoch [8/8], Step [90/157], Loss: 0.4094\n",
            "Epoch [8/8], Step [100/157], Loss: 0.6001\n",
            "Epoch [8/8], Step [110/157], Loss: 0.6457\n",
            "Epoch [8/8], Step [120/157], Loss: 0.6060\n",
            "Epoch [8/8], Step [130/157], Loss: 0.6569\n",
            "Epoch [8/8], Step [140/157], Loss: 0.4777\n",
            "Epoch [8/8], Step [150/157], Loss: 0.6663\n",
            "Test Accuracy of the model on the 10000 test images: 77.54 %\n"
          ]
        }
      ],
      "source": [
        "# Same training code \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "num_epochs = 8\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize model\n",
        "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
        "model = model.to(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYeCqWt_BBRz"
      },
      "source": [
        "## Exercise 2: Self-Attention with Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l71kP10-45YF"
      },
      "source": [
        "Implement a similar model to exercise 1, except this time your embedded input should be added with the positional encoding. For the purpose of this lab, we will use a learned positional encoding, which will be a trainable embedding. Your positional encodings will be added to the initial transformation of the input.\n",
        "\n",
        "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
        "\n",
        "* **Linear 1**: Linear layer which converts input of shape `(batch_size*sequence_length, input_size)` to input of shape `(batch_size*sequence_length, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
        "\n",
        "* **Add Positional Encoding**: Add a learnable positional encoding of shape `(sequence_length, batch_size, embed_dim)` to input of shape `(sequence_length, batch_size, embed_dim)`, where `pos_embed` is the positional embedding size. The output will be of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, features_dim)` to input of shape `(sequence_length*batch_size, features_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, features_dim)` and outputs a tensor of shape `(sequence_length, batch_size, features_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, features_dim)` to `(batch_size, features_dim)`\n",
        "\n",
        "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, sequence_length*features_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "-xAfP5H2_p6o"
      },
      "outputs": [],
      "source": [
        "# Self-attention with positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        self.input_size = input_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.positional_encoding = nn.Parameter(torch.rand(self.seq_length, self.input_size))\n",
        "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.avgpool = nn.AvgPool1d(kernel_size=seq_length)\n",
        "        self.linear3 = nn.Linear(embed_dim, num_classes)   \n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape # 64, 28, 28\n",
        "        for i in range(batch_size):\n",
        "            x[i]=x[i]+self.positional_encoding\n",
        "        input=x.reshape(batch_size*sequence_length, -1) # 1792, 28\n",
        "\n",
        "        l1_out=self.linear1(input) # 1792, 64\n",
        "        l1_out=l1_out.reshape(batch_size,sequence_length, -1) # 64, 28, 64\n",
        "        l1_out=l1_out.permute(1,0,2) # 28, 64, 64 \n",
        "\n",
        "        #pe_out = self.positional_encoding.unsqueeze(1).repeat(1, batch_size, 1) # 28, 64, 28\n",
        "\n",
        "        a1_out, _=self.attention(l1_out, l1_out, l1_out)\n",
        "        a1_out=a1_out.permute(1,0,2) # 64, 28, 64\n",
        "        a1_out=a1_out.reshape(batch_size*sequence_length, -1) # 1792, 64 \n",
        "        \n",
        "        relu1_out=self.relu(a1_out) # 1792, 64\n",
        "        \n",
        "        l2_out=self.linear2(relu1_out)\n",
        "\n",
        "        relu2_out=self.relu(l2_out) # 1792, 64 \n",
        "        relu2_out=relu2_out.reshape(batch_size, sequence_length, -1) # 64, 28, 64\n",
        "        relu2_out=relu2_out.permute(1,0,2) # 28, 64, 64\n",
        "\n",
        "        a2_out, _=self.attention(relu2_out, relu2_out, relu2_out) # 1792, 64\n",
        "        a2_out=a2_out.permute(1, 0, 2) # 64, 28, 64\n",
        "        a2_out=a2_out.reshape(batch_size, sequence_length, -1) # 1792, 64\n",
        "        a2_out=a2_out.permute(0, 2, 1) # 64, 64, 28\n",
        "\n",
        "        relu3_out=self.relu(a2_out) # 64, 64, 28\n",
        "        \n",
        "        avgpool_out=self.avgpool(relu3_out).squeeze() # 64, 64\n",
        "        l3_out=self.linear3(avgpool_out) # 64, 10\n",
        "        return l3_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IKf9WDBS4Y"
      },
      "source": [
        "Use the same training code as the one from part 1 to train your model. You may copy the training loop here. Expect to see close to `~90+%` test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "kxsHnOzXBk95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/8], Step [10/157], Loss: 2.3080\n",
            "Epoch [1/8], Step [20/157], Loss: 2.2362\n",
            "Epoch [1/8], Step [30/157], Loss: 2.1281\n",
            "Epoch [1/8], Step [40/157], Loss: 2.0270\n",
            "Epoch [1/8], Step [50/157], Loss: 2.0658\n",
            "Epoch [1/8], Step [60/157], Loss: 1.9943\n",
            "Epoch [1/8], Step [70/157], Loss: 1.8028\n",
            "Epoch [1/8], Step [80/157], Loss: 1.7964\n",
            "Epoch [1/8], Step [90/157], Loss: 1.6519\n",
            "Epoch [1/8], Step [100/157], Loss: 1.6729\n",
            "Epoch [1/8], Step [110/157], Loss: 1.5191\n",
            "Epoch [1/8], Step [120/157], Loss: 1.1571\n",
            "Epoch [1/8], Step [130/157], Loss: 1.1786\n",
            "Epoch [1/8], Step [140/157], Loss: 1.3048\n",
            "Epoch [1/8], Step [150/157], Loss: 1.1641\n",
            "Epoch [2/8], Step [10/157], Loss: 1.3100\n",
            "Epoch [2/8], Step [20/157], Loss: 1.1102\n",
            "Epoch [2/8], Step [30/157], Loss: 0.8163\n",
            "Epoch [2/8], Step [40/157], Loss: 0.7992\n",
            "Epoch [2/8], Step [50/157], Loss: 0.7793\n",
            "Epoch [2/8], Step [60/157], Loss: 0.7339\n",
            "Epoch [2/8], Step [70/157], Loss: 0.8414\n",
            "Epoch [2/8], Step [80/157], Loss: 0.7955\n",
            "Epoch [2/8], Step [90/157], Loss: 0.6519\n",
            "Epoch [2/8], Step [100/157], Loss: 0.8286\n",
            "Epoch [2/8], Step [110/157], Loss: 0.4260\n",
            "Epoch [2/8], Step [120/157], Loss: 0.5824\n",
            "Epoch [2/8], Step [130/157], Loss: 0.7125\n",
            "Epoch [2/8], Step [140/157], Loss: 0.5897\n",
            "Epoch [2/8], Step [150/157], Loss: 0.6990\n",
            "Epoch [3/8], Step [10/157], Loss: 0.4409\n",
            "Epoch [3/8], Step [20/157], Loss: 0.5969\n",
            "Epoch [3/8], Step [30/157], Loss: 0.3652\n",
            "Epoch [3/8], Step [40/157], Loss: 0.5446\n",
            "Epoch [3/8], Step [50/157], Loss: 0.5357\n",
            "Epoch [3/8], Step [60/157], Loss: 0.2991\n",
            "Epoch [3/8], Step [70/157], Loss: 0.5821\n",
            "Epoch [3/8], Step [80/157], Loss: 0.4854\n",
            "Epoch [3/8], Step [90/157], Loss: 0.3104\n",
            "Epoch [3/8], Step [100/157], Loss: 0.2107\n",
            "Epoch [3/8], Step [110/157], Loss: 0.5561\n",
            "Epoch [3/8], Step [120/157], Loss: 0.5118\n",
            "Epoch [3/8], Step [130/157], Loss: 0.5580\n",
            "Epoch [3/8], Step [140/157], Loss: 0.2601\n",
            "Epoch [3/8], Step [150/157], Loss: 0.1975\n",
            "Epoch [4/8], Step [10/157], Loss: 0.2935\n",
            "Epoch [4/8], Step [20/157], Loss: 0.3384\n",
            "Epoch [4/8], Step [30/157], Loss: 0.4056\n",
            "Epoch [4/8], Step [40/157], Loss: 0.3852\n",
            "Epoch [4/8], Step [50/157], Loss: 0.4882\n",
            "Epoch [4/8], Step [60/157], Loss: 0.3175\n",
            "Epoch [4/8], Step [70/157], Loss: 0.1462\n",
            "Epoch [4/8], Step [80/157], Loss: 0.1619\n",
            "Epoch [4/8], Step [90/157], Loss: 0.2607\n",
            "Epoch [4/8], Step [100/157], Loss: 0.1872\n",
            "Epoch [4/8], Step [110/157], Loss: 0.5545\n",
            "Epoch [4/8], Step [120/157], Loss: 0.2168\n",
            "Epoch [4/8], Step [130/157], Loss: 0.4085\n",
            "Epoch [4/8], Step [140/157], Loss: 0.4577\n",
            "Epoch [4/8], Step [150/157], Loss: 0.5314\n",
            "Epoch [5/8], Step [10/157], Loss: 0.0768\n",
            "Epoch [5/8], Step [20/157], Loss: 0.2784\n",
            "Epoch [5/8], Step [30/157], Loss: 0.2626\n",
            "Epoch [5/8], Step [40/157], Loss: 0.2730\n",
            "Epoch [5/8], Step [50/157], Loss: 0.1499\n",
            "Epoch [5/8], Step [60/157], Loss: 0.2604\n",
            "Epoch [5/8], Step [70/157], Loss: 0.1520\n",
            "Epoch [5/8], Step [80/157], Loss: 0.1323\n",
            "Epoch [5/8], Step [90/157], Loss: 0.1853\n",
            "Epoch [5/8], Step [100/157], Loss: 0.1651\n",
            "Epoch [5/8], Step [110/157], Loss: 0.1566\n",
            "Epoch [5/8], Step [120/157], Loss: 0.3483\n",
            "Epoch [5/8], Step [130/157], Loss: 0.0964\n",
            "Epoch [5/8], Step [140/157], Loss: 0.1651\n",
            "Epoch [5/8], Step [150/157], Loss: 0.2590\n",
            "Epoch [6/8], Step [10/157], Loss: 0.2550\n",
            "Epoch [6/8], Step [20/157], Loss: 0.4580\n",
            "Epoch [6/8], Step [30/157], Loss: 0.1782\n",
            "Epoch [6/8], Step [40/157], Loss: 0.1885\n",
            "Epoch [6/8], Step [50/157], Loss: 0.2361\n",
            "Epoch [6/8], Step [60/157], Loss: 0.2653\n",
            "Epoch [6/8], Step [70/157], Loss: 0.3885\n",
            "Epoch [6/8], Step [80/157], Loss: 0.1829\n",
            "Epoch [6/8], Step [90/157], Loss: 0.1460\n",
            "Epoch [6/8], Step [100/157], Loss: 0.2599\n",
            "Epoch [6/8], Step [110/157], Loss: 0.1111\n",
            "Epoch [6/8], Step [120/157], Loss: 0.2736\n",
            "Epoch [6/8], Step [130/157], Loss: 0.2615\n",
            "Epoch [6/8], Step [140/157], Loss: 0.1536\n",
            "Epoch [6/8], Step [150/157], Loss: 0.2370\n",
            "Epoch [7/8], Step [10/157], Loss: 0.0294\n",
            "Epoch [7/8], Step [20/157], Loss: 0.3864\n",
            "Epoch [7/8], Step [30/157], Loss: 0.1680\n",
            "Epoch [7/8], Step [40/157], Loss: 0.2653\n",
            "Epoch [7/8], Step [50/157], Loss: 0.1296\n",
            "Epoch [7/8], Step [60/157], Loss: 0.1477\n",
            "Epoch [7/8], Step [70/157], Loss: 0.1475\n",
            "Epoch [7/8], Step [80/157], Loss: 0.1824\n",
            "Epoch [7/8], Step [90/157], Loss: 0.0887\n",
            "Epoch [7/8], Step [100/157], Loss: 0.1949\n",
            "Epoch [7/8], Step [110/157], Loss: 0.1048\n",
            "Epoch [7/8], Step [120/157], Loss: 0.3778\n",
            "Epoch [7/8], Step [130/157], Loss: 0.0570\n",
            "Epoch [7/8], Step [140/157], Loss: 0.1201\n",
            "Epoch [7/8], Step [150/157], Loss: 0.1044\n",
            "Epoch [8/8], Step [10/157], Loss: 0.2169\n",
            "Epoch [8/8], Step [20/157], Loss: 0.1166\n",
            "Epoch [8/8], Step [30/157], Loss: 0.0509\n",
            "Epoch [8/8], Step [40/157], Loss: 0.1508\n",
            "Epoch [8/8], Step [50/157], Loss: 0.1538\n",
            "Epoch [8/8], Step [60/157], Loss: 0.3041\n",
            "Epoch [8/8], Step [70/157], Loss: 0.0435\n",
            "Epoch [8/8], Step [80/157], Loss: 0.0855\n",
            "Epoch [8/8], Step [90/157], Loss: 0.1400\n",
            "Epoch [8/8], Step [100/157], Loss: 0.1639\n",
            "Epoch [8/8], Step [110/157], Loss: 0.1380\n",
            "Epoch [8/8], Step [120/157], Loss: 0.1583\n",
            "Epoch [8/8], Step [130/157], Loss: 0.0890\n",
            "Epoch [8/8], Step [140/157], Loss: 0.1910\n",
            "Epoch [8/8], Step [150/157], Loss: 0.1221\n",
            "Test Accuracy of the model on the 10000 test images: 90.87 %\n"
          ]
        }
      ],
      "source": [
        "# Same training code \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "num_epochs = 8\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize model\n",
        "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
