{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMvnHJy2VAh7"
      },
      "source": [
        "# Lab 7: Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnc8ygodCf88"
      },
      "source": [
        "This lab covers the following topics:\n",
        "\n",
        "- Gain insight into the self-attention operation using the sequential MNIST example from before.\n",
        "- Gain insight into positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcGa4Zk7CSO6"
      },
      "source": [
        "## 0 Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafW18o5aPa9"
      },
      "source": [
        "Run the code cell below to download the MNIST digits dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hov-36duZyzP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
            "���������ļ���\n",
            "tar: Error opening archive: Failed to open 'MNIST.tar.gz'\n"
          ]
        }
      ],
      "source": [
        "!wget -O MNIST.tar.gz https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('./', download=True , transform=transforms.Compose([transforms.ToTensor()]), train=True)\n",
        "train_indices = torch.arange(0, 10000)\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "\n",
        "dataset=torchvision.datasets.MNIST('./', download=True , transform=transforms.Compose([transforms.ToTensor()]), train=False)\n",
        "test_indices = torch.arange(0, 10000)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n",
        "                                          shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ-OsDod2fvG"
      },
      "source": [
        "## Exercise 1: Self-Attention without Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w7IP5RKSqOz"
      },
      "source": [
        "In this section, will implement a very simple model based on self-attention without positional encoding. The model you will implement will consider the input image as a sequence of 28 rows. You may use PyTorch's [`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for this part. Implement a model with the following architecture:\n",
        "\n",
        "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
        "\n",
        "* **Linear 1**: Linear layer which converts input of shape `(sequence_length*batch_size, input_size)` to input of shape `(sequence_length*batch_size, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
        "\n",
        "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`. \n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, embed_dim)` to input of shape `(sequence_length*batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, embed_dim)` to `(batch_size, embed_dim)`\n",
        "\n",
        "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, embed_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n",
        "\n",
        "\n",
        "**NOTE**: Be cautious of correctly permuting and reshaping the input between layers. E.g. if `x` is of shape `(batch_size, sequence_length, input_size)`, note that `x.reshape(batch_size*sequence_length, -1) != x.permute(1,0,2).reshape(batch_size*sequence_length, -1)`. In this example, `x.reshape(batch_size*sequence_length, -1)` has `[batch0_seq0, batch0_seq1, ..., batch1_seq0, batch1_seq1, ...]` format, while `x.permute(1,0,2).reshape(batch_size*sequence_length, -1)` has `[batch0_seq0, batch1_seq0, ..., batch0_seq1, batch1_seq1, ...]` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "class MultiHead_Attn(nn.Module):\n",
        "    def __init__(self, embed_dim ,num_head ):        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed_dim = embed_dim ##1024\n",
        "        self.num_head = num_head ##8\n",
        "        self.head_dim = self.embed_dim // self.num_head\n",
        "        \n",
        "        \n",
        "        assert self.embed_dim%self.num_head == 0\n",
        "        \n",
        "        self.q = nn.Linear(self.embed_dim , self.embed_dim )\n",
        "        self.k = nn.Linear(self.embed_dim , self.embed_dim )\n",
        "        self.v = nn.Linear(self.embed_dim , self.embed_dim)\n",
        "        \n",
        "        self.f_linear = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "        self.dropout = nn.Dropout(.35)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "    \n",
        "    def forward(self, x ):\n",
        "        batch_size = x.shape[0]\n",
        "        src_len    = x.shape[1]\n",
        "#         print('src_len=',src_len)\n",
        "        \n",
        "        w_k = self.k(x)\n",
        "        w_q = self.q(x)        \n",
        "        w_v = self.v(x)\n",
        "        \n",
        "        w_q = w_q.view(batch_size,-1,self.head_dim)\n",
        "        w_k = w_k.view(batch_size,-1,self.head_dim)\n",
        "        w_v = w_v.view(batch_size,-1,self.head_dim)\n",
        "         \n",
        "        energy = torch.matmul( w_k.permute(0,2,1) ,w_q )\n",
        "        energy = energy/self.scale\n",
        "        energy = torch.softmax(energy,-1)\n",
        "        \n",
        "        \n",
        "        f_energy = torch.matmul( self.dropout(energy) , w_v.permute(0,2,1))\n",
        "        f_energy = f_energy.permute(0, 2, 1)\n",
        "        f_energy = f_energy.reshape(batch_size,-1)\n",
        "        out = self.f_linear(f_energy)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Self-attention without positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        self.input_size = input_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
        "        self.attention1=MultiHead_Attn(embed_dim, 8)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attention2=MultiHead_Attn(embed_dim, 8)\n",
        "        self.linear3 = nn.Linear(embed_dim*seq_length, 10)     \n",
        "        self.avgpool=nn.AvgPool2d((seq_length, 1), stride=(2, 1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape\n",
        "        input=x.reshape(batch_size*sequence_length, -1)\n",
        "        l1_out=self.linear1(input)\n",
        "        a1_out=self.attention1(l1_out) \n",
        "        relu1_out=self.relu(a1_out)        \n",
        "        print(type(relu1_out))\n",
        "        l2_out=self.linear2(relu1_out)\n",
        "        print(l2_out.shape)\n",
        "        relu2_out=self.relu(l2_out)\n",
        "        a2_out=self.attention2(relu2_out)\n",
        "        print(a2_out.shape)\n",
        "        relu3_out=self.relu(a2_out)\n",
        "        print(relu3_out.shape)\n",
        "        relu3_out=relu3_out.reshape(batch_size,sequence_length, -1) \n",
        "        print(relu3_out.shape)\n",
        "        avgpool_out=self.avgpool(relu3_out)\n",
        "        print(avgpool_out.shape)\n",
        "        avgpool_out=avgpool_out.reshape(batch_size, -1) \n",
        "        print(avgpool_out.shape)\n",
        "        l3_out=self.linear2(avgpool_out)\n",
        "        return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "ppwhZ19Ff9FC"
      },
      "outputs": [],
      "source": [
        "# Self-attention without positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        self.input_size = input_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.avgpool = nn.AvgPool2d((seq_length, 1), stride=(2, 1))\n",
        "        self.linear3 = nn.Linear(embed_dim, num_classes)     \n",
        "\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape # 64, 28, 28\n",
        "        input=x.reshape(batch_size*sequence_length, -1) # 1792, 28\n",
        "        l1_out=self.linear1(input) # 1792, 64\n",
        "        a1_out, _=self.attention(l1_out, l1_out, l1_out) \n",
        "        relu1_out=self.relu(a1_out) # 1792, 64       \n",
        "        l2_out=self.linear2(relu1_out)\n",
        "        relu2_out=self.relu(l2_out) # 1792, 64 \n",
        "        a2_out, _=self.attention(relu2_out, relu2_out, relu2_out) # 1792, 64\n",
        "        relu3_out=self.relu(a2_out) # 1792, 64\n",
        "        relu3_out=relu3_out.reshape(batch_size,sequence_length, -1) # 64, 28, 64\n",
        "        avgpool_out=self.avgpool(relu3_out) # 64, 1, 64\n",
        "        avgpool_out=avgpool_out.reshape(batch_size, -1) # 64, 64\n",
        "        l3_out=self.linear3(avgpool_out) # 64, 10\n",
        "        return l3_out\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zu1c88kgKDZ"
      },
      "source": [
        "Train and evaluate your model by running the cell below. Expect to see  `60-80%` test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "z3FlSD16S8Nh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/8], Step [10/157], Loss: 2.3077\n",
            "Epoch [1/8], Step [20/157], Loss: 2.3170\n",
            "Epoch [1/8], Step [30/157], Loss: 2.2960\n",
            "Epoch [1/8], Step [40/157], Loss: 2.3117\n",
            "Epoch [1/8], Step [50/157], Loss: 2.3046\n",
            "Epoch [1/8], Step [60/157], Loss: 2.2965\n",
            "Epoch [1/8], Step [70/157], Loss: 2.3052\n",
            "Epoch [1/8], Step [80/157], Loss: 2.2832\n",
            "Epoch [1/8], Step [90/157], Loss: 2.2882\n",
            "Epoch [1/8], Step [100/157], Loss: 2.2970\n",
            "Epoch [1/8], Step [110/157], Loss: 2.3073\n",
            "Epoch [1/8], Step [120/157], Loss: 2.3039\n",
            "Epoch [1/8], Step [130/157], Loss: 2.2880\n",
            "Epoch [1/8], Step [140/157], Loss: 2.3068\n",
            "Epoch [1/8], Step [150/157], Loss: 2.2962\n",
            "Epoch [2/8], Step [10/157], Loss: 2.2915\n",
            "Epoch [2/8], Step [20/157], Loss: 2.3048\n",
            "Epoch [2/8], Step [30/157], Loss: 2.3106\n",
            "Epoch [2/8], Step [40/157], Loss: 2.2986\n",
            "Epoch [2/8], Step [50/157], Loss: 2.3147\n",
            "Epoch [2/8], Step [60/157], Loss: 2.3039\n",
            "Epoch [2/8], Step [70/157], Loss: 2.3216\n",
            "Epoch [2/8], Step [80/157], Loss: 2.2995\n",
            "Epoch [2/8], Step [90/157], Loss: 2.2994\n",
            "Epoch [2/8], Step [100/157], Loss: 2.3124\n",
            "Epoch [2/8], Step [110/157], Loss: 2.3168\n",
            "Epoch [2/8], Step [120/157], Loss: 2.3089\n",
            "Epoch [2/8], Step [130/157], Loss: 2.3033\n",
            "Epoch [2/8], Step [140/157], Loss: 2.3034\n",
            "Epoch [2/8], Step [150/157], Loss: 2.3032\n",
            "Epoch [3/8], Step [10/157], Loss: 2.3091\n",
            "Epoch [3/8], Step [20/157], Loss: 2.2907\n",
            "Epoch [3/8], Step [30/157], Loss: 2.3076\n",
            "Epoch [3/8], Step [40/157], Loss: 2.2937\n",
            "Epoch [3/8], Step [50/157], Loss: 2.2946\n",
            "Epoch [3/8], Step [60/157], Loss: 2.3011\n",
            "Epoch [3/8], Step [70/157], Loss: 2.2980\n",
            "Epoch [3/8], Step [80/157], Loss: 2.3133\n",
            "Epoch [3/8], Step [90/157], Loss: 2.2963\n",
            "Epoch [3/8], Step [100/157], Loss: 2.3156\n",
            "Epoch [3/8], Step [110/157], Loss: 2.2989\n",
            "Epoch [3/8], Step [120/157], Loss: 2.3111\n",
            "Epoch [3/8], Step [130/157], Loss: 2.3043\n",
            "Epoch [3/8], Step [140/157], Loss: 2.2904\n",
            "Epoch [3/8], Step [150/157], Loss: 2.2972\n",
            "Epoch [4/8], Step [10/157], Loss: 2.2879\n",
            "Epoch [4/8], Step [20/157], Loss: 2.2880\n",
            "Epoch [4/8], Step [30/157], Loss: 2.2965\n",
            "Epoch [4/8], Step [40/157], Loss: 2.3107\n",
            "Epoch [4/8], Step [50/157], Loss: 2.3101\n",
            "Epoch [4/8], Step [60/157], Loss: 2.3071\n",
            "Epoch [4/8], Step [70/157], Loss: 2.2931\n",
            "Epoch [4/8], Step [80/157], Loss: 2.3127\n",
            "Epoch [4/8], Step [90/157], Loss: 2.2946\n",
            "Epoch [4/8], Step [100/157], Loss: 2.3041\n",
            "Epoch [4/8], Step [110/157], Loss: 2.2860\n",
            "Epoch [4/8], Step [120/157], Loss: 2.3032\n",
            "Epoch [4/8], Step [130/157], Loss: 2.3031\n",
            "Epoch [4/8], Step [140/157], Loss: 2.2917\n",
            "Epoch [4/8], Step [150/157], Loss: 2.3047\n",
            "Epoch [5/8], Step [10/157], Loss: 2.2892\n",
            "Epoch [5/8], Step [20/157], Loss: 2.2982\n",
            "Epoch [5/8], Step [30/157], Loss: 2.2887\n",
            "Epoch [5/8], Step [40/157], Loss: 2.3150\n",
            "Epoch [5/8], Step [50/157], Loss: 2.2973\n",
            "Epoch [5/8], Step [60/157], Loss: 2.3048\n",
            "Epoch [5/8], Step [70/157], Loss: 2.3303\n",
            "Epoch [5/8], Step [80/157], Loss: 2.3013\n",
            "Epoch [5/8], Step [90/157], Loss: 2.3061\n",
            "Epoch [5/8], Step [100/157], Loss: 2.3000\n",
            "Epoch [5/8], Step [110/157], Loss: 2.3061\n",
            "Epoch [5/8], Step [120/157], Loss: 2.2975\n",
            "Epoch [5/8], Step [130/157], Loss: 2.2981\n",
            "Epoch [5/8], Step [140/157], Loss: 2.2997\n",
            "Epoch [5/8], Step [150/157], Loss: 2.3170\n",
            "Epoch [6/8], Step [10/157], Loss: 2.3104\n",
            "Epoch [6/8], Step [20/157], Loss: 2.2891\n",
            "Epoch [6/8], Step [30/157], Loss: 2.3091\n",
            "Epoch [6/8], Step [40/157], Loss: 2.3153\n",
            "Epoch [6/8], Step [50/157], Loss: 2.3085\n",
            "Epoch [6/8], Step [60/157], Loss: 2.3052\n",
            "Epoch [6/8], Step [70/157], Loss: 2.3023\n",
            "Epoch [6/8], Step [80/157], Loss: 2.3109\n",
            "Epoch [6/8], Step [90/157], Loss: 2.3060\n",
            "Epoch [6/8], Step [100/157], Loss: 2.3135\n",
            "Epoch [6/8], Step [110/157], Loss: 2.3069\n",
            "Epoch [6/8], Step [120/157], Loss: 2.2912\n",
            "Epoch [6/8], Step [130/157], Loss: 2.2923\n",
            "Epoch [6/8], Step [140/157], Loss: 2.2886\n",
            "Epoch [6/8], Step [150/157], Loss: 2.2976\n",
            "Epoch [7/8], Step [10/157], Loss: 2.2857\n",
            "Epoch [7/8], Step [20/157], Loss: 2.3052\n",
            "Epoch [7/8], Step [30/157], Loss: 2.2932\n",
            "Epoch [7/8], Step [40/157], Loss: 2.2996\n",
            "Epoch [7/8], Step [50/157], Loss: 2.2997\n",
            "Epoch [7/8], Step [60/157], Loss: 2.3056\n",
            "Epoch [7/8], Step [70/157], Loss: 2.2849\n",
            "Epoch [7/8], Step [80/157], Loss: 2.2894\n",
            "Epoch [7/8], Step [90/157], Loss: 2.3012\n",
            "Epoch [7/8], Step [100/157], Loss: 2.3140\n",
            "Epoch [7/8], Step [110/157], Loss: 2.3024\n",
            "Epoch [7/8], Step [120/157], Loss: 2.2938\n",
            "Epoch [7/8], Step [130/157], Loss: 2.3013\n",
            "Epoch [7/8], Step [140/157], Loss: 2.2941\n",
            "Epoch [7/8], Step [150/157], Loss: 2.3055\n",
            "Epoch [8/8], Step [10/157], Loss: 2.3024\n",
            "Epoch [8/8], Step [20/157], Loss: 2.3082\n",
            "Epoch [8/8], Step [30/157], Loss: 2.3004\n",
            "Epoch [8/8], Step [40/157], Loss: 2.3020\n",
            "Epoch [8/8], Step [50/157], Loss: 2.2985\n",
            "Epoch [8/8], Step [60/157], Loss: 2.2807\n",
            "Epoch [8/8], Step [70/157], Loss: 2.3182\n",
            "Epoch [8/8], Step [80/157], Loss: 2.3022\n",
            "Epoch [8/8], Step [90/157], Loss: 2.2919\n",
            "Epoch [8/8], Step [100/157], Loss: 2.2839\n",
            "Epoch [8/8], Step [110/157], Loss: 2.2964\n",
            "Epoch [8/8], Step [120/157], Loss: 2.2752\n",
            "Epoch [8/8], Step [130/157], Loss: 2.3071\n",
            "Epoch [8/8], Step [140/157], Loss: 2.2861\n",
            "Epoch [8/8], Step [150/157], Loss: 2.2982\n",
            "Test Accuracy of the model on the 10000 test images: 11.35 %\n"
          ]
        }
      ],
      "source": [
        "# Same training code \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "num_epochs = 8\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize model\n",
        "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
        "model = model.to(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYeCqWt_BBRz"
      },
      "source": [
        "## Exercise 2: Self-Attention with Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l71kP10-45YF"
      },
      "source": [
        "Implement a similar model to exercise 1, except this time your embedded input should be added with the positional encoding. For the purpose of this lab, we will use a learned positional encoding, which will be a trainable embedding. Your positional encodings will be added to the initial transformation of the input.\n",
        "\n",
        "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
        "\n",
        "* **Linear 1**: Linear layer which converts input of shape `(batch_size*sequence_length, input_size)` to input of shape `(batch_size*sequence_length, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
        "\n",
        "* **Add Positional Encoding**: Add a learnable positional encoding of shape `(sequence_length, batch_size, embed_dim)` to input of shape `(sequence_length, batch_size, embed_dim)`, where `pos_embed` is the positional embedding size. The output will be of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, features_dim)` to input of shape `(sequence_length*batch_size, features_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, features_dim)` and outputs a tensor of shape `(sequence_length, batch_size, features_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, features_dim)` to `(batch_size, features_dim)`\n",
        "\n",
        "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, sequence_length*features_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-xAfP5H2_p6o"
      },
      "outputs": [],
      "source": [
        "# Self-attention with positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        self.positional_encoding = nn.Parameter(torch.rand(self.seq_length, self.embed_dim))\n",
        "        self.input_size = input_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.seq_length = seq_length\n",
        "        self.num_classes = num_classes\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.positional_encoding = nn.Parameter(torch.rand(self.seq_length, self.input_size))\n",
        "        self.linear1 = nn.Linear(input_size, embed_dim)\n",
        "        self.attention1=MultiHead_Attn(embed_dim, 8)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.linear2 = nn.Linear(embed_dim, embed_dim)\n",
        "        self.attention2=MultiHead_Attn(embed_dim, 8)\n",
        "        self.linear3 = nn.Linear(embed_dim*seq_length, 10)     \n",
        "        self.avgpool=nn.AvgPool2d((seq_length, 1), stride=(2, 1))\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape\n",
        "        for i in range(batch_size):\n",
        "            x[i]=x[i]+self.positional_encoding\n",
        "         \n",
        "        \n",
        "        input=x.reshape(batch_size*sequence_length, -1)        \n",
        "        l1_out=self.linear1(input)\n",
        "        a1_out=self.attention1(l1_out) \n",
        "        relu1_out=self.relu(a1_out)        \n",
        "#         print(type(relu1_out))\n",
        "        l2_out=self.linear2(relu1_out)\n",
        "#         print(l2_out.shape)\n",
        "        relu2_out=self.relu(l2_out)\n",
        "        a2_out=self.attention2(relu2_out)\n",
        "#         print(a2_out.shape)\n",
        "        relu3_out=self.relu(a2_out)\n",
        "#         print(relu3_out.shape)\n",
        "        relu3_out=relu3_out.reshape(batch_size,sequence_length, -1) \n",
        "#         print(relu3_out.shape)\n",
        "        avgpool_out=self.avgpool(relu3_out)\n",
        "        avgpool_out=avgpool_out.reshape(batch_size, -1) \n",
        "#         print(avgpool_out.shape)\n",
        "        l3_out=self.linear2(avgpool_out)\n",
        "        return l3_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IKf9WDBS4Y"
      },
      "source": [
        "Use the same training code as the one from part 1 to train your model. You may copy the training loop here. Expect to see close to `~90+%` test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kxsHnOzXBk95"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'myModel' object has no attribute 'seq_length'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.005\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[39m# Initialize model\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m model \u001b[39m=\u001b[39m myModel(input_size\u001b[39m=\u001b[39;49minput_size, embed_dim\u001b[39m=\u001b[39;49mhidden_size, seq_length\u001b[39m=\u001b[39;49msequence_length)\n\u001b[0;32m     23\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Loss and optimizer\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[21], line 10\u001b[0m, in \u001b[0;36mmyModel.__init__\u001b[1;34m(self, input_size, embed_dim, seq_length, num_classes, num_heads)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39msuper\u001b[39m(myModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[39m# TODO: Initialize myModel\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mParameter(torch\u001b[39m.\u001b[39mrand(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim))\n\u001b[0;32m     11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m input_size\n\u001b[0;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim \u001b[39m=\u001b[39m embed_dim\n",
            "File \u001b[1;32mc:\\Users\\leoqi\\anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'myModel' object has no attribute 'seq_length'"
          ]
        }
      ],
      "source": [
        "# Same training code \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "num_epochs = 8\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize model\n",
        "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
